{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# week 4"
      ],
      "metadata": {
        "id": "xPn6HQwFlffZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Now access your folder\n",
        "import os\n",
        "main_data_folder = '/content/drive/MyDrive/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SO2RwUJkWi7",
        "outputId": "189d6958-496f-41b1-9b00-82715ffe4fa7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rioxarray"
      ],
      "metadata": {
        "id": "sVRKoYC_eGz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# COMMON DATA LOADER\n",
        "\n",
        "Loads and prepares data for all three correction models:\n",
        "  1. Bias Correction Model\n",
        "  2. Machine Learning Predictor\n",
        "  3. Statistical Regression Model\n",
        "\n",
        "This module provides a unified data loading pipeline ensuring consistency\n",
        "across all modeling approaches.\n",
        "\n",
        "### LOading master_dataset.csv\n",
        "**The file master_dataset.csv is produced during the week3.**"
      ],
      "metadata": {
        "id": "Bu5Q05JF235U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzhtcB_Ci6u0",
        "outputId": "a58fff2d-67e4-4627-f17f-e6b42994b59f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "WEEK 4: COMMON DATA LOADER\n",
            "================================================================================\n",
            "Loading data from: /content/drive/MyDrive/GenHack2025/week3_quantitative_metrics\n",
            "Output directory: week4_explanatory_models\n",
            "\n",
            "================================================================================\n",
            "STEP 1: LOADING MASTER DATASET\n",
            "================================================================================\n",
            "‚úì Loaded master dataset: 286 stations\n",
            "  Time period: 2020-2023\n",
            "  Cities: 5\n",
            "  Total days analyzed: 384,977\n",
            "\n",
            "--- Dataset Overview ---\n",
            "Countries: ['DEU' 'FRA' 'ITA' 'POL' 'ESP']\n",
            "Cities: ['Berlin' 'Paris' 'Milano' 'Warszawa' 'Madrid']\n",
            "\n",
            "Station distribution:\n",
            "category\n",
            "Rural       243\n",
            "Suburban     22\n",
            "Urban        21\n",
            "Name: count, dtype: int64\n",
            "\n",
            "City distribution:\n",
            "city\n",
            "Milano      88\n",
            "Madrid      87\n",
            "Berlin      86\n",
            "Warszawa    20\n",
            "Paris        5\n",
            "Name: count, dtype: int64\n",
            "\n",
            "================================================================================\n",
            "STEP 2: DATA QUALITY CHECKS\n",
            "================================================================================\n",
            "\n",
            "--- Missing Values Check ---\n",
            "‚úì No missing values in critical columns\n",
            "\n",
            "--- Outlier Detection ---\n",
            "Bias range: -6.55¬∞C to 2.73¬∞C\n",
            "RMSE range: 1.11¬∞C to 6.96¬∞C\n",
            "Elevation range: 1m to 2472m\n",
            "\n",
            "‚ö†Ô∏è  10 potential bias outliers detected (>3 œÉ):\n",
            "  Station 17644: LAGDEI (bias: 2.38¬∞C, elevation: 1252m)\n",
            "  Station 17826: TERUZZI (bias: 2.43¬∞C, elevation: 1077m)\n",
            "  Station 26129: DAONE (DIGA DI MALGA BISSINA) (bias: -4.96¬∞C, elevation: 1792m)\n",
            "  Station 26131: DAONE (DIGA DI PONTE MORANDIN) (bias: -5.81¬∞C, elevation: 720m)\n",
            "  Station 26132: DAONE (MALGA BISSINA) (bias: -4.96¬∞C, elevation: 1785m)\n",
            "  Station 26251: STORO (bias: -6.55¬∞C, elevation: 385m)\n",
            "  Station 26265: TREMALZO (bias: 2.38¬∞C, elevation: 1560m)\n",
            "  Station 232: PUERTO DE NAVACERRADA (bias: 2.73¬∞C, elevation: 1893m)\n",
            "  Station 27143: LA PINILLA  ESTACION DE ESQUI (bias: 2.54¬∞C, elevation: 1798m)\n",
            "  Station 27237: PUERTO ALTO DEL LEON (bias: 2.36¬∞C, elevation: 1532m)\n",
            "\n",
            "================================================================================\n",
            "STEP 3: FEATURE ENGINEERING\n",
            "================================================================================\n",
            "‚úì Created seasonal bias features\n",
            "\n",
            "‚úì Created 9 new features\n",
            "\n",
            "--- New Feature Summary ---\n",
            "\n",
            "Elevation categories:\n",
            "elevation_category\n",
            "Lowland      104\n",
            "Low_Hills     46\n",
            "Hills         23\n",
            "Mountains    113\n",
            "Name: count, dtype: int64\n",
            "\n",
            "NDVI categories:\n",
            "ndvi_category\n",
            "Very_Low      21\n",
            "Low           84\n",
            "Medium       147\n",
            "High          34\n",
            "Very_High      0\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Coastal distribution:\n",
            "coastal\n",
            "False    283\n",
            "True       3\n",
            "Name: count, dtype: int64\n",
            "\n",
            "================================================================================\n",
            "STEP 4: TRAIN-TEST SPLIT\n",
            "================================================================================\n",
            "\n",
            "Split strategy: Leave-one-city-out\n",
            "  Training cities: ['Berlin', 'Milano', 'Warszawa', 'Madrid']\n",
            "  Test city: ['Paris']\n",
            "\n",
            "‚úì Training set: 281 stations (98.3%)\n",
            "‚úì Test set: 5 stations (1.7%)\n",
            "\n",
            "Test set composition:\n",
            "  Categories: {'Rural': 4, 'Suburban': 1}\n",
            "  Elevation range: 69m to 151m\n",
            "  Bias range: -1.38¬∞C to -0.60¬∞C\n",
            "\n",
            "================================================================================\n",
            "STEP 5: PREPARING FEATURE SETS\n",
            "================================================================================\n",
            "\n",
            "‚úì Basic features (3): ['elevation', 'lat', 'lon']\n",
            "‚úì ML features (7): ['elevation', 'lat', 'lon', 'mean_station_temp', 'distance_to_city_km', 'distance_to_coast_km', 'ndvi_mean']\n",
            "‚úì Statistical features (4): ['elevation', 'lat', 'lon', 'mean_station_temp']\n",
            "\n",
            "--- Feature Availability Check ---\n",
            "  ‚úì mean_station_temp\n",
            "  ‚úì distance_to_coast_km\n",
            "  ‚úì lon\n",
            "  ‚úì ndvi_mean\n",
            "  ‚úì distance_to_city_km\n",
            "  ‚úì elevation\n",
            "  ‚úì lat\n",
            "\n",
            "================================================================================\n",
            "STEP 6: BASELINE METRICS (Before Correction)\n",
            "================================================================================\n",
            "\n",
            "--- Baseline Performance (No Corrections) ---\n",
            "\n",
            "Training Set (281 stations):\n",
            "  Bias:         -1.258¬∞C\n",
            "  MAE:           1.401¬∞C\n",
            "  RMSE:          1.718¬∞C\n",
            "  Correlation:  0.9216\n",
            "  R¬≤:           0.8493\n",
            "\n",
            "Test Set (5 stations):\n",
            "  Bias:         -1.018¬∞C\n",
            "  MAE:           1.018¬∞C\n",
            "  RMSE:          1.056¬∞C\n",
            "  Correlation:  0.9751\n",
            "  R¬≤:           0.9509\n",
            "\n",
            "================================================================================\n",
            "STEP 7: SAVING PROCESSED DATA\n",
            "================================================================================\n",
            "\n",
            "‚úì Saved processed datasets:\n",
            "  - processed_data_full.csv (286 stations)\n",
            "  - processed_data_train.csv (281 stations)\n",
            "  - processed_data_test.csv (5 stations)\n",
            "  - feature_sets.json\n",
            "  - baseline_metrics.csv\n",
            "\n",
            "================================================================================\n",
            "STEP 8: SUMMARY STATISTICS\n",
            "================================================================================\n",
            "\n",
            "--- Overall Dataset Summary ---\n",
            "Total Stations           :        286\n",
            "Training Stations        :        281\n",
            "Test Stations            :          5\n",
            "Total Days Analyzed      :     384977\n",
            "Cities                   :          5\n",
            "Mean ERA5 Bias           :     -1.254\n",
            "Bias Std Dev             :      1.162\n",
            "Overall RMSE             :      2.048\n",
            "Overall MAE              :      1.733\n",
            "Overall Correlation      :      0.988\n",
            "\n",
            "--- Seasonal Bias Summary ---\n",
            "Winter    :  -0.780 ¬± 1.196 ¬∞C (n=285)\n",
            "Spring    :  -1.394 ¬± 1.252 ¬∞C (n=285)\n",
            "Summer    :  -1.689 ¬± 1.327 ¬∞C (n=283)\n",
            "Fall      :  -1.005 ¬± 1.139 ¬∞C (n=282)\n",
            "\n",
            "--- City-Level Summary ---\n",
            "          Bias_Mean  Bias_Std   RMSE  Elevation_Mean  N_Stations\n",
            "city                                                            \n",
            "Berlin       -0.982     0.201  1.490          64.826          86\n",
            "Madrid       -1.299     1.200  2.097         887.552          87\n",
            "Milano       -1.590     1.640  2.674         510.205          88\n",
            "Paris        -1.018     0.317  1.589         103.200           5\n",
            "Warszawa     -0.809     0.347  1.588         134.950          20\n",
            "\n",
            "================================================================================\n",
            "DATA LOADING COMPLETE\n",
            "================================================================================\n",
            "\n",
            "‚úÖ All data loaded and prepared successfully!\n",
            "\n",
            "Ready for modeling:\n",
            "  üìä 281 training stations\n",
            "  üß™ 5 test stations\n",
            "  üìÅ 9 engineered features\n",
            "  üìà Baseline RMSE: 1.718¬∞C\n",
            "\n",
            "Next steps:\n",
            "  1. Run Model 1: Bias Correction (Simple)\n",
            "  2. Run Model 2: Machine Learning Predictor\n",
            "  3. Run Model 3: Statistical Regression\n",
            "\n",
            "================================================================================\n",
            "\n",
            "üí° Data objects available for modeling:\n",
            "   - df_model: Full dataset with engineered features\n",
            "   - df_train: Training subset\n",
            "   - df_test: Test subset\n",
            "   - baseline_train: Training baseline metrics\n",
            "   - baseline_test: Test baseline metrics\n",
            "   - ml_features: List of features for ML models\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "\n",
        "DATA_DIR = \"/content/drive/MyDrive/GenHack2025/week3_quantitative_metrics\"\n",
        "OUTPUT_DIR = \"week4_explanatory_models\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"WEEK 4: COMMON DATA LOADER\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Loading data from: {DATA_DIR}\")\n",
        "print(f\"Output directory: {OUTPUT_DIR}\")\n",
        "\n",
        "# ============================================================================\n",
        "# LOAD MASTER DATASET\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 1: LOADING MASTER DATASET\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Load main dataset from Week 3\n",
        "master_file = f\"{DATA_DIR}/master_dataset.csv\"\n",
        "\n",
        "if not os.path.exists(master_file):\n",
        "    raise FileNotFoundError(\n",
        "        f\"\\n‚ùå ERROR: Master dataset not found!\\n\"\n",
        "        f\"Expected: {master_file}\\n\\n\"\n",
        "        f\"Please run Week 3 analysis first to generate the master dataset.\"\n",
        "    )\n",
        "\n",
        "df_master = pd.read_csv(master_file)\n",
        "\n",
        "print(f\"‚úì Loaded master dataset: {len(df_master)} stations\")\n",
        "print(f\"  Time period: 2020-2023\")\n",
        "print(f\"  Cities: {df_master['city'].nunique()}\")\n",
        "print(f\"  Total days analyzed: {df_master['n_days'].sum():,}\")\n",
        "\n",
        "# Display basic statistics\n",
        "print(\"\\n--- Dataset Overview ---\")\n",
        "print(f\"Countries: {df_master['country'].unique()}\")\n",
        "print(f\"Cities: {df_master['city'].unique()}\")\n",
        "print(f\"\\nStation distribution:\")\n",
        "print(df_master['category'].value_counts())\n",
        "print(f\"\\nCity distribution:\")\n",
        "print(df_master['city'].value_counts())\n",
        "\n",
        "# ============================================================================\n",
        "# DATA QUALITY CHECKS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 2: DATA QUALITY CHECKS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Check for missing values in critical columns\n",
        "critical_cols = ['bias', 'mae', 'rmse', 'correlation', 'mean_station_temp',\n",
        "                 'mean_era5_temp', 'elevation', 'ndvi_mean', 'lat', 'lon']\n",
        "\n",
        "print(\"\\n--- Missing Values Check ---\")\n",
        "missing_summary = df_master[critical_cols].isnull().sum()\n",
        "if missing_summary.sum() == 0:\n",
        "    print(\"‚úì No missing values in critical columns\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Missing values detected:\")\n",
        "    print(missing_summary[missing_summary > 0])\n",
        "\n",
        "# Check for outliers\n",
        "print(\"\\n--- Outlier Detection ---\")\n",
        "print(f\"Bias range: {df_master['bias'].min():.2f}¬∞C to {df_master['bias'].max():.2f}¬∞C\")\n",
        "print(f\"RMSE range: {df_master['rmse'].min():.2f}¬∞C to {df_master['rmse'].max():.2f}¬∞C\")\n",
        "print(f\"Elevation range: {df_master['elevation'].min():.0f}m to {df_master['elevation'].max():.0f}m\")\n",
        "\n",
        "# Identify potential outliers (>3 std from mean)\n",
        "bias_outliers = df_master[np.abs(df_master['bias'] - df_master['bias'].mean()) >\n",
        "                          3 * df_master['bias'].std()]\n",
        "if len(bias_outliers) > 0:\n",
        "    print(f\"\\n‚ö†Ô∏è  {len(bias_outliers)} potential bias outliers detected (>3 œÉ):\")\n",
        "    for _, row in bias_outliers.iterrows():\n",
        "        print(f\"  Station {row['station_id']}: {row['station_name'][:30]} \"\n",
        "              f\"(bias: {row['bias']:.2f}¬∞C, elevation: {row['elevation']:.0f}m)\")\n",
        "\n",
        "# ============================================================================\n",
        "# FEATURE ENGINEERING\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 3: FEATURE ENGINEERING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Create derived features for modeling\n",
        "df_model = df_master.copy()\n",
        "\n",
        "# 1. Absolute error\n",
        "df_model['abs_bias'] = df_model['bias'].abs()\n",
        "\n",
        "# 2. Relative error (percentage)\n",
        "df_model['relative_error'] = (df_model['bias'] / df_model['mean_station_temp']) * 100\n",
        "\n",
        "# 3. Temperature deviation from mean\n",
        "df_model['temp_deviation'] = df_model['mean_station_temp'] - df_model['mean_station_temp'].mean()\n",
        "\n",
        "# 4. Elevation categories\n",
        "df_model['elevation_category'] = pd.cut(\n",
        "    df_model['elevation'],\n",
        "    bins=[-np.inf, 100, 300, 600, np.inf],\n",
        "    labels=['Lowland', 'Low_Hills', 'Hills', 'Mountains']\n",
        ")\n",
        "\n",
        "# 5. Distance category\n",
        "df_model['distance_category'] = pd.cut(\n",
        "    df_model['distance_to_city_km'],\n",
        "    bins=[0, 15, 40, 150],\n",
        "    labels=['Urban', 'Suburban', 'Rural'],\n",
        "    include_lowest=True\n",
        ")\n",
        "\n",
        "# 6. NDVI categories\n",
        "df_model['ndvi_category'] = pd.cut(\n",
        "    df_model['ndvi_mean'],\n",
        "    bins=[-1, 0.2, 0.4, 0.6, 0.8, 1.0],\n",
        "    labels=['Very_Low', 'Low', 'Medium', 'High', 'Very_High']\n",
        ")\n",
        "\n",
        "# 7. Seasonal bias features (from individual season columns)\n",
        "seasonal_cols = ['bias_Winter', 'bias_Spring', 'bias_Summer', 'bias_Fall']\n",
        "if all(col in df_model.columns for col in seasonal_cols):\n",
        "    df_model['seasonal_range'] = (\n",
        "        df_model[seasonal_cols].max(axis=1) -\n",
        "        df_model[seasonal_cols].min(axis=1)\n",
        "    )\n",
        "    df_model['peak_season_bias'] = df_model[seasonal_cols].max(axis=1)\n",
        "    print(\"‚úì Created seasonal bias features\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Seasonal bias columns not found\")\n",
        "\n",
        "# 8. Coastal vs inland\n",
        "coastal_threshold = 200  # km\n",
        "df_model['coastal'] = df_model['distance_to_coast_km'] < coastal_threshold\n",
        "\n",
        "print(f\"\\n‚úì Created {len([c for c in df_model.columns if c not in df_master.columns])} new features\")\n",
        "\n",
        "# Display new features\n",
        "print(\"\\n--- New Feature Summary ---\")\n",
        "if 'elevation_category' in df_model.columns:\n",
        "    print(f\"\\nElevation categories:\")\n",
        "    print(df_model['elevation_category'].value_counts().sort_index())\n",
        "\n",
        "if 'ndvi_category' in df_model.columns:\n",
        "    print(f\"\\nNDVI categories:\")\n",
        "    print(df_model['ndvi_category'].value_counts().sort_index())\n",
        "\n",
        "if 'coastal' in df_model.columns:\n",
        "    print(f\"\\nCoastal distribution:\")\n",
        "    print(df_model['coastal'].value_counts())\n",
        "\n",
        "# ============================================================================\n",
        "# TRAIN-TEST SPLIT\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 4: TRAIN-TEST SPLIT\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Strategy: Hold out one city for testing to ensure geographic generalization\n",
        "test_cities = ['Paris']  # Small dataset, good test case\n",
        "train_cities = [c for c in df_model['city'].unique() if c not in test_cities]\n",
        "\n",
        "print(f\"\\nSplit strategy: Leave-one-city-out\")\n",
        "print(f\"  Training cities: {train_cities}\")\n",
        "print(f\"  Test city: {test_cities}\")\n",
        "\n",
        "df_train = df_model[df_model['city'].isin(train_cities)].copy()\n",
        "df_test = df_model[df_model['city'].isin(test_cities)].copy()\n",
        "\n",
        "print(f\"\\n‚úì Training set: {len(df_train)} stations ({len(df_train)/len(df_model)*100:.1f}%)\")\n",
        "print(f\"‚úì Test set: {len(df_test)} stations ({len(df_test)/len(df_model)*100:.1f}%)\")\n",
        "\n",
        "# Verify test set has representation across categories\n",
        "if len(df_test) > 0:\n",
        "    print(f\"\\nTest set composition:\")\n",
        "    print(f\"  Categories: {df_test['category'].value_counts().to_dict()}\")\n",
        "    print(f\"  Elevation range: {df_test['elevation'].min():.0f}m to {df_test['elevation'].max():.0f}m\")\n",
        "    print(f\"  Bias range: {df_test['bias'].min():.2f}¬∞C to {df_test['bias'].max():.2f}¬∞C\")\n",
        "\n",
        "# ============================================================================\n",
        "# PREPARE FEATURE SETS FOR MODELING\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 5: PREPARING FEATURE SETS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Define feature sets for different models\n",
        "\n",
        "# Basic features (for simple correction model)\n",
        "basic_features = ['elevation', 'lat', 'lon']\n",
        "\n",
        "# Extended features (for ML model)\n",
        "ml_features = [\n",
        "    'elevation', 'lat', 'lon',\n",
        "    'mean_station_temp',\n",
        "    'distance_to_city_km',\n",
        "    'distance_to_coast_km',\n",
        "    'ndvi_mean'\n",
        "]\n",
        "\n",
        "# Statistical model features (for regression with seasonal data)\n",
        "stat_features = basic_features + ['mean_station_temp']\n",
        "\n",
        "print(f\"\\n‚úì Basic features ({len(basic_features)}): {basic_features}\")\n",
        "print(f\"‚úì ML features ({len(ml_features)}): {ml_features}\")\n",
        "print(f\"‚úì Statistical features ({len(stat_features)}): {stat_features}\")\n",
        "\n",
        "# Check feature availability\n",
        "print(\"\\n--- Feature Availability Check ---\")\n",
        "all_features = list(set(basic_features + ml_features + stat_features))\n",
        "for feat in all_features:\n",
        "    if feat in df_model.columns:\n",
        "        print(f\"  ‚úì {feat}\")\n",
        "    else:\n",
        "        print(f\"  ‚úó {feat} - MISSING!\")\n",
        "\n",
        "# ============================================================================\n",
        "# COMPUTE BASELINE METRICS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 6: BASELINE METRICS (Before Correction)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def calculate_metrics(actual, predicted, label=\"\"):\n",
        "    \"\"\"Calculate standard error metrics\"\"\"\n",
        "    errors = predicted - actual\n",
        "\n",
        "    metrics = {\n",
        "        'label': label,\n",
        "        'bias': errors.mean(),\n",
        "        'mae': np.abs(errors).mean(),\n",
        "        'rmse': np.sqrt((errors ** 2).mean()),\n",
        "        'std': errors.std(),\n",
        "        'r': np.corrcoef(actual, predicted)[0, 1],\n",
        "        'r2': np.corrcoef(actual, predicted)[0, 1] ** 2,\n",
        "        'n': len(actual)\n",
        "    }\n",
        "    return metrics\n",
        "\n",
        "# Calculate baseline (ERA5 without any correction)\n",
        "baseline_train = calculate_metrics(\n",
        "    df_train['mean_station_temp'],\n",
        "    df_train['mean_era5_temp'],\n",
        "    'Training Set Baseline'\n",
        ")\n",
        "\n",
        "baseline_test = calculate_metrics(\n",
        "    df_test['mean_station_temp'],\n",
        "    df_test['mean_era5_temp'],\n",
        "    'Test Set Baseline'\n",
        ")\n",
        "\n",
        "print(\"\\n--- Baseline Performance (No Corrections) ---\")\n",
        "print(f\"\\nTraining Set ({len(df_train)} stations):\")\n",
        "print(f\"  Bias:        {baseline_train['bias']:>+7.3f}¬∞C\")\n",
        "print(f\"  MAE:         {baseline_train['mae']:>7.3f}¬∞C\")\n",
        "print(f\"  RMSE:        {baseline_train['rmse']:>7.3f}¬∞C\")\n",
        "print(f\"  Correlation: {baseline_train['r']:>7.4f}\")\n",
        "print(f\"  R¬≤:          {baseline_train['r2']:>7.4f}\")\n",
        "\n",
        "if len(df_test) > 0:\n",
        "    print(f\"\\nTest Set ({len(df_test)} stations):\")\n",
        "    print(f\"  Bias:        {baseline_test['bias']:>+7.3f}¬∞C\")\n",
        "    print(f\"  MAE:         {baseline_test['mae']:>7.3f}¬∞C\")\n",
        "    print(f\"  RMSE:        {baseline_test['rmse']:>7.3f}¬∞C\")\n",
        "    print(f\"  Correlation: {baseline_test['r']:>7.4f}\")\n",
        "    print(f\"  R¬≤:          {baseline_test['r2']:>7.4f}\")\n",
        "\n",
        "# ============================================================================\n",
        "# SAVE PROCESSED DATA\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 7: SAVING PROCESSED DATA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Save processed datasets\n",
        "df_model.to_csv(f\"{OUTPUT_DIR}/processed_data_full.csv\", index=False)\n",
        "df_train.to_csv(f\"{OUTPUT_DIR}/processed_data_train.csv\", index=False)\n",
        "df_test.to_csv(f\"{OUTPUT_DIR}/processed_data_test.csv\", index=False)\n",
        "\n",
        "print(f\"\\n‚úì Saved processed datasets:\")\n",
        "print(f\"  - processed_data_full.csv ({len(df_model)} stations)\")\n",
        "print(f\"  - processed_data_train.csv ({len(df_train)} stations)\")\n",
        "print(f\"  - processed_data_test.csv ({len(df_test)} stations)\")\n",
        "\n",
        "# Save feature lists\n",
        "feature_info = {\n",
        "    'basic_features': basic_features,\n",
        "    'ml_features': ml_features,\n",
        "    'stat_features': stat_features\n",
        "}\n",
        "\n",
        "import json\n",
        "with open(f\"{OUTPUT_DIR}/feature_sets.json\", 'w') as f:\n",
        "    json.dump(feature_info, f, indent=2)\n",
        "\n",
        "print(f\"  - feature_sets.json\")\n",
        "\n",
        "# Save baseline metrics\n",
        "baseline_metrics = pd.DataFrame([baseline_train, baseline_test])\n",
        "baseline_metrics.to_csv(f\"{OUTPUT_DIR}/baseline_metrics.csv\", index=False)\n",
        "print(f\"  - baseline_metrics.csv\")\n",
        "\n",
        "# ============================================================================\n",
        "# SUMMARY STATISTICS FOR REPORT\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 8: SUMMARY STATISTICS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "summary_stats = {\n",
        "    'Total Stations': len(df_model),\n",
        "    'Training Stations': len(df_train),\n",
        "    'Test Stations': len(df_test),\n",
        "    'Total Days Analyzed': int(df_model['n_days'].sum()),\n",
        "    'Cities': df_model['city'].nunique(),\n",
        "    'Mean ERA5 Bias': df_model['bias'].mean(),\n",
        "    'Bias Std Dev': df_model['bias'].std(),\n",
        "    'Overall RMSE': df_model['rmse'].mean(),\n",
        "    'Overall MAE': df_model['mae'].mean(),\n",
        "    'Overall Correlation': df_model['correlation'].mean()\n",
        "}\n",
        "\n",
        "print(\"\\n--- Overall Dataset Summary ---\")\n",
        "for key, value in summary_stats.items():\n",
        "    if isinstance(value, float):\n",
        "        print(f\"{key:<25}: {value:>10.3f}\")\n",
        "    else:\n",
        "        print(f\"{key:<25}: {value:>10}\")\n",
        "\n",
        "# Seasonal summary\n",
        "if all(f'bias_{s}' in df_model.columns for s in ['Winter', 'Spring', 'Summer', 'Fall']):\n",
        "    print(\"\\n--- Seasonal Bias Summary ---\")\n",
        "    for season in ['Winter', 'Spring', 'Summer', 'Fall']:\n",
        "        col = f'bias_{season}'\n",
        "        seasonal_bias = df_model[col].dropna()\n",
        "        if len(seasonal_bias) > 0:\n",
        "            print(f\"{season:<10}: {seasonal_bias.mean():>+7.3f} ¬± {seasonal_bias.std():<6.3f}¬∞C \"\n",
        "                  f\"(n={len(seasonal_bias)})\")\n",
        "\n",
        "# Geographic summary\n",
        "print(\"\\n--- City-Level Summary ---\")\n",
        "city_summary = df_model.groupby('city').agg({\n",
        "    'bias': ['mean', 'std'],\n",
        "    'rmse': 'mean',\n",
        "    'elevation': 'mean',\n",
        "    'station_id': 'count'\n",
        "}).round(3)\n",
        "city_summary.columns = ['Bias_Mean', 'Bias_Std', 'RMSE', 'Elevation_Mean', 'N_Stations']\n",
        "print(city_summary)\n",
        "\n",
        "# ============================================================================\n",
        "# DATA LOADING COMPLETE\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DATA LOADING COMPLETE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\n‚úÖ All data loaded and prepared successfully!\")\n",
        "print(f\"\\nReady for modeling:\")\n",
        "print(f\"  üìä {len(df_train)} training stations\")\n",
        "print(f\"  üß™ {len(df_test)} test stations\")\n",
        "print(f\"  üìÅ {len([c for c in df_model.columns if c not in df_master.columns])} engineered features\")\n",
        "print(f\"  üìà Baseline RMSE: {baseline_train['rmse']:.3f}¬∞C\")\n",
        "\n",
        "print(f\"\\nNext steps:\")\n",
        "print(f\"  1. Run Model 1: Bias Correction (Simple)\")\n",
        "print(f\"  2. Run Model 2: Machine Learning Predictor\")\n",
        "print(f\"  3. Run Model 3: Statistical Regression\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "# Make data available for next scripts\n",
        "print(\"\\nüí° Data objects available for modeling:\")\n",
        "print(\"   - df_model: Full dataset with engineered features\")\n",
        "print(\"   - df_train: Training subset\")\n",
        "print(\"   - df_test: Test subset\")\n",
        "print(\"   - baseline_train: Training baseline metrics\")\n",
        "print(\"   - baseline_test: Test baseline metrics\")\n",
        "print(\"   - ml_features: List of features for ML models\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 1: Simple, practical correction model based on Week 3 findings:\n",
        "  - Seasonal adjustment\n",
        "  - Elevation adjustment\n",
        "  - Temperature-dependent scaling (optional)\n",
        "\n",
        "Goal: Reduce ERA5 RMSE by 20-30% with easy-to-apply corrections"
      ],
      "metadata": {
        "id": "GYHhlLhWfPV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.dpi'] = 150\n",
        "plt.rcParams['savefig.dpi'] = 300\n",
        "plt.rcParams['font.size'] = 11\n",
        "\n",
        "# ============================================================================\n",
        "# LOAD DATA\n",
        "# ============================================================================\n",
        "\n",
        "WEEK3_DIR =\"/content/drive/MyDrive/GenHack2025/week3_quantitative_metrics\"\n",
        "WEEK4_DIR = \"week4_explanatory_model1\"\n",
        "\n",
        "os.makedirs(WEEK4_DIR, exist_ok=True)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"WEEK 4 - MODEL 1: OPERATIONAL BIAS CORRECTION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Load master dataset\n",
        "df = pd.read_csv(f\"{WEEK3_DIR}/master_dataset.csv\")\n",
        "print(f\"\\nLoaded {len(df)} stations from Week 3\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 1: CALCULATE CORRECTION COMPONENTS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"STEP 1: CALCULATING CORRECTION COMPONENTS\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Base bias correction (overall mean)\n",
        "base_correction = -df['bias'].mean()  # Negate because bias is (ERA5 - Observed)\n",
        "print(f\"\\n1. Base Correction: +{base_correction:.3f}¬∞C\")\n",
        "print(f\"   (Overall mean ERA5 bias: {df['bias'].mean():.3f}¬∞C)\")\n",
        "\n",
        "# Seasonal corrections\n",
        "print(\"\\n2. Seasonal Corrections:\")\n",
        "seasonal_data = []\n",
        "for season in ['Winter', 'Spring', 'Summer', 'Fall']:\n",
        "    col_name = f'bias_{season}'\n",
        "    if col_name in df.columns:\n",
        "        season_bias = df[col_name].mean()\n",
        "        season_correction = base_correction - (-season_bias)  # Adjustment from base\n",
        "        seasonal_data.append({\n",
        "            'Season': season,\n",
        "            'Mean_Bias': season_bias,\n",
        "            'Correction': -season_bias,\n",
        "            'Adjustment_from_Base': season_correction\n",
        "        })\n",
        "        print(f\"   {season:<8}: bias={season_bias:+.3f}¬∞C ‚Üí correction={-season_bias:+.3f}¬∞C\")\n",
        "\n",
        "df_seasonal = pd.DataFrame(seasonal_data)\n",
        "\n",
        "# Elevation correction (linear regression)\n",
        "print(\"\\n3. Elevation Correction:\")\n",
        "valid_elev = df[['elevation', 'bias']].dropna()\n",
        "slope, intercept, r_value, p_value, std_err = stats.linregress(\n",
        "    valid_elev['elevation'],\n",
        "    valid_elev['bias']\n",
        ")\n",
        "\n",
        "print(f\"   Linear regression: bias = {intercept:.4f} + {slope:.6f} √ó elevation\")\n",
        "print(f\"   R¬≤ = {r_value**2:.4f}, p-value = {p_value:.4f}\")\n",
        "print(f\"   Elevation coefficient: {slope:.6f}¬∞C/m\")\n",
        "\n",
        "if p_value < 0.05:\n",
        "    print(f\"   ‚úì Statistically significant\")\n",
        "    elevation_slope = slope\n",
        "else:\n",
        "    print(f\"   ‚úó Not statistically significant - using zero slope\")\n",
        "    elevation_slope = 0.0\n",
        "\n",
        "# Temperature-dependent correction (multiplicative effect)\n",
        "print(\"\\n4. Temperature-Dependent Scaling:\")\n",
        "temp_corr, temp_p = stats.pearsonr(df['bias'].dropna(), df['mean_station_temp'].dropna())\n",
        "print(f\"   Correlation (bias vs temperature): r={temp_corr:.4f}, p={temp_p:.6f}\")\n",
        "\n",
        "if abs(temp_corr) > 0.2 and temp_p < 0.05:\n",
        "    print(f\"   ‚úì Multiplicative effect detected\")\n",
        "    temp_slope, temp_intercept, _, _, _ = stats.linregress(\n",
        "        df['mean_station_temp'].dropna(),\n",
        "        df['bias'].dropna()\n",
        "    )\n",
        "    print(f\"   Temperature coefficient: {temp_slope:.4f}¬∞C per ¬∞C\")\n",
        "    use_temp_correction = True\n",
        "else:\n",
        "    print(f\"   ‚úó No significant temperature dependence\")\n",
        "    temp_slope = 0.0\n",
        "    use_temp_correction = False\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 2: DEFINE CORRECTION FUNCTION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"STEP 2: DEFINING CORRECTION FUNCTION\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "def apply_correction(era5_temp, season, elevation, observed_temp=None, method='full'):\n",
        "    \"\"\"\n",
        "    Apply operational bias correction to ERA5 temperature\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    era5_temp : float or array\n",
        "        Raw ERA5 temperature (¬∞C)\n",
        "    season : str or array\n",
        "        Season name ('Winter', 'Spring', 'Summer', 'Fall')\n",
        "    elevation : float or array\n",
        "        Station elevation (meters)\n",
        "    observed_temp : float or array, optional\n",
        "        Observed temperature for temperature-dependent correction\n",
        "    method : str\n",
        "        'base_only' - only base correction\n",
        "        'seasonal' - base + seasonal\n",
        "        'full' - base + seasonal + elevation\n",
        "        'full_temp' - base + seasonal + elevation + temperature-dependent\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    corrected_temp : float or array\n",
        "        Corrected ERA5 temperature\n",
        "    \"\"\"\n",
        "    correction = np.zeros_like(era5_temp) if isinstance(era5_temp, np.ndarray) else 0.0\n",
        "\n",
        "    # Base correction (always applied)\n",
        "    correction += base_correction\n",
        "\n",
        "    if method in ['seasonal', 'full', 'full_temp']:\n",
        "        # Seasonal adjustment\n",
        "        if isinstance(season, str):\n",
        "            season_adj = df_seasonal[df_seasonal['Season'] == season]['Adjustment_from_Base'].values\n",
        "            if len(season_adj) > 0:\n",
        "                correction += season_adj[0]\n",
        "        else:\n",
        "            # Array of seasons\n",
        "            for s in df_seasonal['Season'].unique():\n",
        "                mask = season == s\n",
        "                season_adj = df_seasonal[df_seasonal['Season'] == s]['Adjustment_from_Base'].values\n",
        "                if len(season_adj) > 0:\n",
        "                    correction = np.where(mask, correction + season_adj[0], correction)\n",
        "\n",
        "    if method in ['full', 'full_temp']:\n",
        "        # Elevation adjustment\n",
        "        correction += elevation_slope * elevation\n",
        "\n",
        "    if method == 'full_temp' and use_temp_correction and observed_temp is not None:\n",
        "        # Temperature-dependent adjustment\n",
        "        correction += temp_slope * observed_temp\n",
        "\n",
        "    return era5_temp + correction\n",
        "\n",
        "print(\"\\n‚úì Correction function defined\")\n",
        "print(f\"   Available methods: 'base_only', 'seasonal', 'full', 'full_temp'\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 3: APPLY CORRECTIONS TO DATASET\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"STEP 3: APPLYING CORRECTIONS\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Determine season for each station (use most common from seasonal data)\n",
        "def get_dominant_season(row):\n",
        "    seasons = []\n",
        "    for season in ['Winter', 'Spring', 'Summer', 'Fall']:\n",
        "        if f'n_days_{season}' in row.index and row[f'n_days_{season}'] > 0:\n",
        "            seasons.append((season, row[f'n_days_{season}']))\n",
        "    if seasons:\n",
        "        return max(seasons, key=lambda x: x[1])[0]\n",
        "    return 'Summer'  # Default\n",
        "\n",
        "df['season'] = df.apply(get_dominant_season, axis=1)\n",
        "\n",
        "# Apply different correction methods\n",
        "print(\"\\nApplying correction methods:\")\n",
        "\n",
        "methods = {\n",
        "    'base_only': 'Base correction only',\n",
        "    'seasonal': 'Base + Seasonal',\n",
        "    'full': 'Base + Seasonal + Elevation',\n",
        "}\n",
        "\n",
        "if use_temp_correction:\n",
        "    methods['full_temp'] = 'Base + Seasonal + Elevation + Temperature'\n",
        "\n",
        "for method_name, description in methods.items():\n",
        "    df[f'corrected_{method_name}'] = apply_correction(\n",
        "        df['mean_era5_temp'],\n",
        "        df['season'],\n",
        "        df['elevation'],\n",
        "        df['mean_station_temp'],\n",
        "        method=method_name\n",
        "    )\n",
        "\n",
        "    # Calculate new error\n",
        "    df[f'error_{method_name}'] = df[f'corrected_{method_name}'] - df['mean_station_temp']\n",
        "    df[f'abs_error_{method_name}'] = np.abs(df[f'error_{method_name}'])\n",
        "\n",
        "    print(f\"  ‚úì {method_name}: {description}\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 4: EVALUATE IMPROVEMENTS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"STEP 4: EVALUATING CORRECTION PERFORMANCE\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Original errors\n",
        "original_bias = df['bias'].mean()\n",
        "original_mae = df['mae'].mean()\n",
        "original_rmse = df['rmse'].mean()\n",
        "\n",
        "print(f\"\\nOriginal ERA5 Performance:\")\n",
        "print(f\"  Bias:  {original_bias:+.3f}¬∞C\")\n",
        "print(f\"  MAE:   {original_mae:.3f}¬∞C\")\n",
        "print(f\"  RMSE:  {original_rmse:.3f}¬∞C\")\n",
        "\n",
        "print(f\"\\n{'Method':<20} {'Bias (¬∞C)':<12} {'MAE (¬∞C)':<12} {'RMSE (¬∞C)':<12} {'Improvement':<12}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "results = []\n",
        "\n",
        "for method_name, description in methods.items():\n",
        "    new_bias = df[f'error_{method_name}'].mean()\n",
        "    new_mae = df[f'abs_error_{method_name}'].mean()\n",
        "    new_rmse = np.sqrt((df[f'error_{method_name}']**2).mean())\n",
        "\n",
        "    improvement = ((original_rmse - new_rmse) / original_rmse) * 100\n",
        "\n",
        "    print(f\"{method_name:<20} {new_bias:>+6.3f}      {new_mae:>6.3f}      \"\n",
        "          f\"{new_rmse:>6.3f}      {improvement:>+5.1f}%\")\n",
        "\n",
        "    results.append({\n",
        "        'Method': description,\n",
        "        'Method_Code': method_name,\n",
        "        'Bias': new_bias,\n",
        "        'MAE': new_mae,\n",
        "        'RMSE': new_rmse,\n",
        "        'RMSE_Improvement_%': improvement,\n",
        "        'Original_RMSE': original_rmse\n",
        "    })\n",
        "\n",
        "df_results = pd.DataFrame(results)\n",
        "\n",
        "# Save results\n",
        "df_results.to_csv(f\"{WEEK4_DIR}/model1_correction_performance.csv\", index=False)\n",
        "print(f\"\\n‚úì Results saved to: {WEEK4_DIR}/model1_correction_performance.csv\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 5: VISUALIZATIONS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"STEP 5: CREATING VISUALIZATIONS\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Visualization 1: Before vs After Scatter Plots\n",
        "print(\"\\nCreating Visualization 1: Before/After Comparison...\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
        "\n",
        "methods_to_plot = ['base_only', 'seasonal', 'full']\n",
        "if use_temp_correction:\n",
        "    methods_to_plot.append('full_temp')\n",
        "\n",
        "for idx, (ax, method) in enumerate(zip(axes.flat, ['original'] + methods_to_plot[:3])):\n",
        "    if method == 'original':\n",
        "        x = df['mean_station_temp']\n",
        "        y = df['mean_era5_temp']\n",
        "        title_suffix = 'Original ERA5'\n",
        "        rmse_val = original_rmse\n",
        "    else:\n",
        "        x = df['mean_station_temp']\n",
        "        y = df[f'corrected_{method}']\n",
        "        method_desc = methods[method]\n",
        "        rmse_val = df_results[df_results['Method_Code'] == method]['RMSE'].values[0]\n",
        "        improvement = df_results[df_results['Method_Code'] == method]['RMSE_Improvement_%'].values[0]\n",
        "        title_suffix = f'{method_desc}\\nRMSE: {rmse_val:.3f}¬∞C ({improvement:+.1f}%)'\n",
        "\n",
        "    # Scatter plot\n",
        "    scatter = ax.scatter(x, y, c=df['elevation'], cmap='terrain',\n",
        "                        s=40, alpha=0.6, edgecolors='black', linewidth=0.5)\n",
        "\n",
        "    # 1:1 line\n",
        "    min_val = min(x.min(), y.min())\n",
        "    max_val = max(x.max(), y.max())\n",
        "    ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2,\n",
        "            label='Perfect Agreement', alpha=0.7)\n",
        "\n",
        "    # Calculate R¬≤\n",
        "    correlation = np.corrcoef(x, y)[0, 1]\n",
        "    r_squared = correlation ** 2\n",
        "\n",
        "    ax.set_xlabel('Observed Temperature (¬∞C)', fontsize=11)\n",
        "    ax.set_ylabel('ERA5 Temperature (¬∞C)', fontsize=11)\n",
        "    ax.set_title(title_suffix, fontsize=12, fontweight='bold')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.legend(fontsize=9)\n",
        "\n",
        "    # Add statistics text\n",
        "    ax.text(0.05, 0.95, f'R¬≤ = {r_squared:.4f}\\nRMSE = {rmse_val:.3f}¬∞C',\n",
        "            transform=ax.transAxes, fontsize=10, verticalalignment='top',\n",
        "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "\n",
        "# Colorbar\n",
        "fig.colorbar(scatter, ax=axes.ravel().tolist(), label='Elevation (m)',\n",
        "             orientation='horizontal', pad=0.05, aspect=40)\n",
        "\n",
        "fig.suptitle('Model 1: Operational Bias Correction Performance',\n",
        "             fontsize=15, fontweight='bold', y=0.995)\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{WEEK4_DIR}/viz1_model1_before_after_scatter.png\", dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(f\"  ‚úì Saved: viz1_model1_before_after_scatter.png\")\n",
        "\n",
        "# Visualization 2: Error Distribution Comparison\n",
        "print(\"\\nCreating Visualization 2: Error Distribution Comparison...\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "for idx, (ax, method) in enumerate(zip(axes.flat, ['original'] + methods_to_plot[:3])):\n",
        "    if method == 'original':\n",
        "        errors = df['bias']\n",
        "        title = 'Original ERA5 Errors'\n",
        "    else:\n",
        "        errors = df[f'error_{method}']\n",
        "        method_desc = methods[method]\n",
        "        improvement = df_results[df_results['Method_Code'] == method]['RMSE_Improvement_%'].values[0]\n",
        "        title = f'{method_desc}\\n(RMSE improvement: {improvement:+.1f}%)'\n",
        "\n",
        "    # Histogram\n",
        "    ax.hist(errors, bins=50, alpha=0.7, color='steelblue', edgecolor='black')\n",
        "\n",
        "    # Add vertical line at zero\n",
        "    ax.axvline(0, color='red', linestyle='--', linewidth=2, label='Zero Error')\n",
        "\n",
        "    # Add mean line\n",
        "    mean_error = errors.mean()\n",
        "    ax.axvline(mean_error, color='orange', linestyle='-', linewidth=2,\n",
        "              label=f'Mean: {mean_error:+.3f}¬∞C')\n",
        "\n",
        "    ax.set_xlabel('Error (Predicted - Observed) ¬∞C', fontsize=11)\n",
        "    ax.set_ylabel('Frequency', fontsize=11)\n",
        "    ax.set_title(title, fontsize=12, fontweight='bold')\n",
        "    ax.legend(fontsize=9)\n",
        "    ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "fig.suptitle('Model 1: Error Distribution Before and After Corrections',\n",
        "             fontsize=15, fontweight='bold', y=0.995)\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{WEEK4_DIR}/viz2_model1_error_distributions.png\", dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(f\"  ‚úì Saved: viz2_model1_error_distributions.png\")\n",
        "\n",
        "# Visualization 3: RMSE Improvement Bar Chart\n",
        "print(\"\\nCreating Visualization 3: RMSE Improvement Comparison...\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 7))\n",
        "\n",
        "methods_display = df_results['Method'].values\n",
        "improvements = df_results['RMSE_Improvement_%'].values\n",
        "rmse_values = df_results['RMSE'].values\n",
        "\n",
        "x_pos = np.arange(len(methods_display))\n",
        "colors = ['#e74c3c' if imp < 0 else '#27ae60' for imp in improvements]\n",
        "\n",
        "bars = ax.bar(x_pos, improvements, color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
        "\n",
        "# Add value labels on bars\n",
        "for i, (bar, rmse, imp) in enumerate(zip(bars, rmse_values, improvements)):\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height + (1 if height > 0 else -1),\n",
        "            f'{imp:+.1f}%\\n({rmse:.3f}¬∞C)',\n",
        "            ha='center', va='bottom' if height > 0 else 'top',\n",
        "            fontsize=10, fontweight='bold')\n",
        "\n",
        "ax.axhline(0, color='black', linestyle='-', linewidth=1)\n",
        "ax.set_ylabel('RMSE Improvement (%)', fontsize=13)\n",
        "ax.set_xlabel('Correction Method', fontsize=13)\n",
        "ax.set_title('Model 1: RMSE Improvement by Correction Method\\n' +\n",
        "             f'Original RMSE: {original_rmse:.3f}¬∞C',\n",
        "             fontsize=14, fontweight='bold')\n",
        "ax.set_xticks(x_pos)\n",
        "ax.set_xticklabels(methods_display, rotation=15, ha='right')\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{WEEK4_DIR}/viz3_model1_rmse_improvement.png\", dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(f\"  ‚úì Saved: viz3_model1_rmse_improvement.png\")\n",
        "\n",
        "# Visualization 4: Correction Components\n",
        "print(\"\\nCreating Visualization 4: Correction Components...\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Panel A: Seasonal corrections\n",
        "ax = axes[0, 0]\n",
        "seasons = df_seasonal['Season'].values\n",
        "corrections = df_seasonal['Correction'].values\n",
        "colors_seasonal = ['#3498db', '#2ecc71', '#e74c3c', '#f39c12']\n",
        "\n",
        "bars = ax.bar(seasons, corrections, color=colors_seasonal, alpha=0.7,\n",
        "              edgecolor='black', linewidth=1.5)\n",
        "for bar, corr in zip(bars, corrections):\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height + 0.05,\n",
        "            f'{corr:+.3f}¬∞C', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "\n",
        "ax.axhline(base_correction, color='red', linestyle='--', linewidth=2,\n",
        "          label=f'Base: {base_correction:+.3f}¬∞C', alpha=0.7)\n",
        "ax.set_ylabel('Correction (¬∞C)', fontsize=11)\n",
        "ax.set_title('A) Seasonal Bias Corrections', fontsize=12, fontweight='bold')\n",
        "ax.legend(fontsize=10)\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Panel B: Elevation effect\n",
        "ax = axes[0, 1]\n",
        "ax.scatter(df['elevation'], df['bias'], alpha=0.5, s=30, c='steelblue', edgecolors='black', linewidth=0.5)\n",
        "\n",
        "# Regression line\n",
        "x_line = np.linspace(df['elevation'].min(), df['elevation'].max(), 100)\n",
        "y_line = intercept + slope * x_line\n",
        "ax.plot(x_line, y_line, 'r-', linewidth=2,\n",
        "        label=f'y = {intercept:.3f} + {slope:.6f}x\\nR¬≤ = {r_value**2:.3f}')\n",
        "\n",
        "ax.axhline(0, color='black', linestyle='--', linewidth=1, alpha=0.5)\n",
        "ax.set_xlabel('Elevation (m)', fontsize=11)\n",
        "ax.set_ylabel('ERA5 Bias (¬∞C)', fontsize=11)\n",
        "ax.set_title('B) Elevation vs Bias Relationship', fontsize=12, fontweight='bold')\n",
        "ax.legend(fontsize=10)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Panel C: Temperature dependence\n",
        "ax = axes[1, 0]\n",
        "if use_temp_correction:\n",
        "    ax.scatter(df['mean_station_temp'], df['bias'], alpha=0.5, s=30,\n",
        "              c='coral', edgecolors='black', linewidth=0.5)\n",
        "\n",
        "    x_temp = np.linspace(df['mean_station_temp'].min(), df['mean_station_temp'].max(), 100)\n",
        "    y_temp = temp_intercept + temp_slope * x_temp\n",
        "    ax.plot(x_temp, y_temp, 'r-', linewidth=2,\n",
        "            label=f'y = {temp_intercept:.3f} + {temp_slope:.4f}x\\nr = {temp_corr:.3f}')\n",
        "\n",
        "    ax.axhline(0, color='black', linestyle='--', linewidth=1, alpha=0.5)\n",
        "    ax.set_xlabel('Mean Observed Temperature (¬∞C)', fontsize=11)\n",
        "    ax.set_ylabel('ERA5 Bias (¬∞C)', fontsize=11)\n",
        "    ax.set_title('C) Temperature Dependence (Multiplicative Effect)', fontsize=12, fontweight='bold')\n",
        "    ax.legend(fontsize=10)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "else:\n",
        "    ax.text(0.5, 0.5, 'No significant\\ntemperature dependence\\ndetected',\n",
        "            ha='center', va='center', transform=ax.transAxes, fontsize=14)\n",
        "    ax.set_title('C) Temperature Dependence', fontsize=12, fontweight='bold')\n",
        "\n",
        "# Panel D: Summary table\n",
        "ax = axes[1, 1]\n",
        "ax.axis('tight')\n",
        "ax.axis('off')\n",
        "\n",
        "summary_data = [\n",
        "    ['Component', 'Value', 'Significance'],\n",
        "    ['Base Correction', f'{base_correction:+.3f}¬∞C', '‚úì Always applied'],\n",
        "    ['Seasonal Range', f'{corrections.max() - corrections.min():.3f}¬∞C',\n",
        "     f'‚úì p<0.001' if len(df_seasonal) > 0 else 'N/A'],\n",
        "    ['Elevation Slope', f'{elevation_slope:.6f}¬∞C/m',\n",
        "     '‚úì Significant' if p_value < 0.05 else '‚úó Not significant'],\n",
        "    ['Temp Coefficient', f'{temp_slope:.4f}¬∞C/¬∞C' if use_temp_correction else 'N/A',\n",
        "     '‚úì Multiplicative' if use_temp_correction else '‚úó Not detected'],\n",
        "    ['', '', ''],\n",
        "    ['Best Method', methods[df_results.iloc[-1]['Method_Code']], ''],\n",
        "    ['RMSE Improvement', f\"{df_results.iloc[-1]['RMSE_Improvement_%']:+.1f}%\", ''],\n",
        "    ['Final RMSE', f\"{df_results.iloc[-1]['RMSE']:.3f}¬∞C\",\n",
        "     f'(was {original_rmse:.3f}¬∞C)']\n",
        "]\n",
        "\n",
        "table = ax.table(cellText=summary_data, cellLoc='left', loc='center',\n",
        "                colWidths=[0.35, 0.25, 0.4])\n",
        "table.auto_set_font_size(False)\n",
        "table.set_fontsize(10)\n",
        "table.scale(1, 2.5)\n",
        "\n",
        "# Style header row\n",
        "for i in range(3):\n",
        "    table[(0, i)].set_facecolor('#34495e')\n",
        "    table[(0, i)].set_text_props(weight='bold', color='white')\n",
        "\n",
        "# Style separator row\n",
        "for i in range(3):\n",
        "    table[(5, i)].set_facecolor('#ecf0f1')\n",
        "\n",
        "ax.set_title('D) Correction Summary', fontsize=12, fontweight='bold', pad=20)\n",
        "\n",
        "fig.suptitle('Model 1: Correction Components Analysis',\n",
        "             fontsize=15, fontweight='bold', y=0.995)\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{WEEK4_DIR}/viz4_model1_correction_components.png\", dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(f\"  ‚úì Saved: viz4_model1_correction_components.png\")\n",
        "\n",
        "# Visualization 5: Error Reduction Map by City\n",
        "print(\"\\nCreating Visualization 5: Error Reduction by City...\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 7))\n",
        "\n",
        "# Use best method\n",
        "best_method = df_results.iloc[-1]['Method_Code']\n",
        "\n",
        "city_improvements = []\n",
        "for city in sorted(df['city'].unique()):\n",
        "    city_data = df[df['city'] == city]\n",
        "    original_city_rmse = city_data['rmse'].mean()\n",
        "    corrected_city_rmse = np.sqrt((city_data[f'error_{best_method}']**2).mean())\n",
        "    improvement = ((original_city_rmse - corrected_city_rmse) / original_city_rmse) * 100\n",
        "\n",
        "    city_improvements.append({\n",
        "        'City': city,\n",
        "        'Original_RMSE': original_city_rmse,\n",
        "        'Corrected_RMSE': corrected_city_rmse,\n",
        "        'Improvement_%': improvement,\n",
        "        'N_Stations': len(city_data)\n",
        "    })\n",
        "\n",
        "df_city_imp = pd.DataFrame(city_improvements)\n",
        "df_city_imp = df_city_imp.sort_values('Improvement_%', ascending=False)\n",
        "\n",
        "x_pos = np.arange(len(df_city_imp))\n",
        "colors_city = ['#27ae60' if imp > 0 else '#e74c3c' for imp in df_city_imp['Improvement_%']]\n",
        "\n",
        "bars = ax.bar(x_pos, df_city_imp['Improvement_%'], color=colors_city,\n",
        "              alpha=0.7, edgecolor='black', linewidth=1.5)\n",
        "\n",
        "for i, (bar, imp, orig, corr) in enumerate(zip(bars,\n",
        "                                                 df_city_imp['Improvement_%'],\n",
        "                                                 df_city_imp['Original_RMSE'],\n",
        "                                                 df_city_imp['Corrected_RMSE'])):\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
        "            f'{imp:+.1f}%\\n{orig:.2f}‚Üí{corr:.2f}¬∞C',\n",
        "            ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
        "\n",
        "ax.axhline(0, color='black', linestyle='-', linewidth=1)\n",
        "ax.set_ylabel('RMSE Improvement (%)', fontsize=13)\n",
        "ax.set_xlabel('City', fontsize=13)\n",
        "ax.set_title(f'Model 1: RMSE Improvement by City\\nUsing {methods[best_method]}',\n",
        "             fontsize=14, fontweight='bold')\n",
        "ax.set_xticks(x_pos)\n",
        "ax.set_xticklabels(df_city_imp['City'], rotation=0)\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{WEEK4_DIR}/viz5_model1_city_improvements.png\", dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(f\"  ‚úì Saved: viz5_model1_city_improvements.png\")\n",
        "\n",
        "# Save city improvements\n",
        "df_city_imp.to_csv(f\"{WEEK4_DIR}/model1_city_improvements.csv\", index=False)\n",
        "\n",
        "# ============================================================================\n",
        "# FINAL SUMMARY\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODEL 1: OPERATIONAL BIAS CORRECTION - COMPLETE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nüìä PERFORMANCE SUMMARY:\")\n",
        "print(f\"  Original RMSE: {original_rmse:.3f}¬∞C\")\n",
        "\n",
        "best_result = df_results.iloc[-1]\n",
        "print(f\"\\n  Best Method: {best_result['Method']}\")\n",
        "print(f\"  Corrected RMSE: {best_result['RMSE']:.3f}¬∞C\")\n",
        "print(f\"  Improvement: {best_result['RMSE_Improvement_%']:+.1f}%\")\n",
        "print(f\"  New Bias: {best_result['Bias']:+.3f}¬∞C (was {original_bias:+.3f}¬∞C)\")\n",
        "\n",
        "print(\"\\nüìÅ OUTPUTS SAVED:\")\n",
        "print(f\"  1. Performance metrics: model1_correction_performance.csv\")\n",
        "print(f\"  2. City improvements: model1_city_improvements.csv\")\n",
        "print(f\"  3. Before/after scatter: viz1_model1_before_after_scatter.png\")\n",
        "print(f\"  4. Error distributions: viz2_model1_error_distributions.png\")\n",
        "print(f\"  5. RMSE improvement: viz3_model1_rmse_improvement.png\")\n",
        "print(f\"  6. Correction components: viz4_model1_correction_components.png\")\n",
        "print(f\"  7. City improvements: viz5_model1_city_improvements.png\")\n",
        "\n",
        "print(\"\\n‚úì Model 1 analysis complete!\")"
      ],
      "metadata": {
        "id": "sDxLEFtxkNI5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7839da16-6ba1-41de-b90e-a911ad8e68b3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "WEEK 4 - MODEL 1: OPERATIONAL BIAS CORRECTION\n",
            "================================================================================\n",
            "\n",
            "Loaded 286 stations from Week 3\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "STEP 1: CALCULATING CORRECTION COMPONENTS\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "1. Base Correction: +1.254¬∞C\n",
            "   (Overall mean ERA5 bias: -1.254¬∞C)\n",
            "\n",
            "2. Seasonal Corrections:\n",
            "   Winter  : bias=-0.780¬∞C ‚Üí correction=+0.780¬∞C\n",
            "   Spring  : bias=-1.394¬∞C ‚Üí correction=+1.394¬∞C\n",
            "   Summer  : bias=-1.689¬∞C ‚Üí correction=+1.689¬∞C\n",
            "   Fall    : bias=-1.005¬∞C ‚Üí correction=+1.005¬∞C\n",
            "\n",
            "3. Elevation Correction:\n",
            "   Linear regression: bias = -1.2823 + 0.000061 √ó elevation\n",
            "   R¬≤ = 0.0006, p-value = 0.6888\n",
            "   Elevation coefficient: 0.000061¬∞C/m\n",
            "   ‚úó Not statistically significant - using zero slope\n",
            "\n",
            "4. Temperature-Dependent Scaling:\n",
            "   Correlation (bias vs temperature): r=-0.3285, p=0.000000\n",
            "   ‚úì Multiplicative effect detected\n",
            "   Temperature coefficient: -0.1278¬∞C per ¬∞C\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "STEP 2: DEFINING CORRECTION FUNCTION\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "‚úì Correction function defined\n",
            "   Available methods: 'base_only', 'seasonal', 'full', 'full_temp'\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "STEP 3: APPLYING CORRECTIONS\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Applying correction methods:\n",
            "  ‚úì base_only: Base correction only\n",
            "  ‚úì seasonal: Base + Seasonal\n",
            "  ‚úì full: Base + Seasonal + Elevation\n",
            "  ‚úì full_temp: Base + Seasonal + Elevation + Temperature\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "STEP 4: EVALUATING CORRECTION PERFORMANCE\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Original ERA5 Performance:\n",
            "  Bias:  -1.254¬∞C\n",
            "  MAE:   1.733¬∞C\n",
            "  RMSE:  2.048¬∞C\n",
            "\n",
            "Method               Bias (¬∞C)    MAE (¬∞C)     RMSE (¬∞C)    Improvement \n",
            "----------------------------------------------------------------------\n",
            "base_only            +0.000       0.733       1.160      +43.4%\n",
            "seasonal             -0.205       0.724       1.183      +42.2%\n",
            "full                 -0.205       0.724       1.183      +42.2%\n",
            "full_temp            -2.462       2.560       2.808      -37.1%\n",
            "\n",
            "‚úì Results saved to: week4_explanatory_model1/model1_correction_performance.csv\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "STEP 5: CREATING VISUALIZATIONS\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Creating Visualization 1: Before/After Comparison...\n",
            "  ‚úì Saved: viz1_model1_before_after_scatter.png\n",
            "\n",
            "Creating Visualization 2: Error Distribution Comparison...\n",
            "  ‚úì Saved: viz2_model1_error_distributions.png\n",
            "\n",
            "Creating Visualization 3: RMSE Improvement Comparison...\n",
            "  ‚úì Saved: viz3_model1_rmse_improvement.png\n",
            "\n",
            "Creating Visualization 4: Correction Components...\n",
            "  ‚úì Saved: viz4_model1_correction_components.png\n",
            "\n",
            "Creating Visualization 5: Error Reduction by City...\n",
            "  ‚úì Saved: viz5_model1_city_improvements.png\n",
            "\n",
            "================================================================================\n",
            "MODEL 1: OPERATIONAL BIAS CORRECTION - COMPLETE\n",
            "================================================================================\n",
            "\n",
            "üìä PERFORMANCE SUMMARY:\n",
            "  Original RMSE: 2.048¬∞C\n",
            "\n",
            "  Best Method: Base + Seasonal + Elevation + Temperature\n",
            "  Corrected RMSE: 2.808¬∞C\n",
            "  Improvement: -37.1%\n",
            "  New Bias: -2.462¬∞C (was -1.254¬∞C)\n",
            "\n",
            "üìÅ OUTPUTS SAVED:\n",
            "  1. Performance metrics: model1_correction_performance.csv\n",
            "  2. City improvements: model1_city_improvements.csv\n",
            "  3. Before/after scatter: viz1_model1_before_after_scatter.png\n",
            "  4. Error distributions: viz2_model1_error_distributions.png\n",
            "  5. RMSE improvement: viz3_model1_rmse_improvement.png\n",
            "  6. Correction components: viz4_model1_correction_components.png\n",
            "  7. City improvements: viz5_model1_city_improvements.png\n",
            "\n",
            "‚úì Model 1 analysis complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 2: Advanced ML approach to predict and correct ERA5 errors:\n",
        "  - Random Forest Regressor (primary model)\n",
        "  - Gradient Boosting (alternative)\n",
        "  - Feature importance analysis\n",
        "  - Cross-validation\n",
        "  - Hold-out test set validation\n",
        "  - Spatial cross-validation (leave-one-city-out)\n",
        "\n",
        "Goal: Learn complex non-linear patterns in ERA5 errors"
      ],
      "metadata": {
        "id": "v0zQEOOSfuAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.dpi'] = 150\n",
        "plt.rcParams['savefig.dpi'] = 300\n",
        "plt.rcParams['font.size'] = 11\n",
        "\n",
        "# ============================================================================\n",
        "# LOAD DATA\n",
        "# ============================================================================\n",
        "\n",
        "WEEK3_DIR =\"/content/drive/MyDrive/GenHack2025/week3_quantitative_metrics\"\n",
        "WEEK4_DIR = \"week4_explanatory_model2\"\n",
        "os.makedirs(WEEK4_DIR, exist_ok=True)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"WEEK 4 - MODEL 2: MACHINE LEARNING ERROR PREDICTOR\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Load master dataset\n",
        "df = pd.read_csv(f\"{WEEK3_DIR}/master_dataset.csv\")\n",
        "print(f\"\\nLoaded {len(df)} stations from Week 3\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 1: FEATURE ENGINEERING\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"STEP 1: FEATURE ENGINEERING\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Define features\n",
        "feature_columns = [\n",
        "    'ndvi_mean',\n",
        "    'elevation',\n",
        "    'lat',\n",
        "    'lon',\n",
        "    'distance_to_city_km',\n",
        "    'distance_to_coast_km',\n",
        "    'mean_station_temp'\n",
        "]\n",
        "\n",
        "# Add season encoding (one-hot)\n",
        "print(\"\\nAdding seasonal features...\")\n",
        "def get_dominant_season(row):\n",
        "    seasons = []\n",
        "    for season in ['Winter', 'Spring', 'Summer', 'Fall']:\n",
        "        if f'n_days_{season}' in row.index and row[f'n_days_{season}'] > 0:\n",
        "            seasons.append((season, row[f'n_days_{season}']))\n",
        "    if seasons:\n",
        "        return max(seasons, key=lambda x: x[1])[0]\n",
        "    return 'Summer'\n",
        "\n",
        "df['season'] = df.apply(get_dominant_season, axis=1)\n",
        "\n",
        "# One-hot encode season\n",
        "season_dummies = pd.get_dummies(df['season'], prefix='season')\n",
        "df = pd.concat([df, season_dummies], axis=1)\n",
        "\n",
        "# Add season columns to features\n",
        "season_features = [col for col in df.columns if col.startswith('season_')]\n",
        "feature_columns.extend(season_features)\n",
        "\n",
        "print(f\"  ‚úì Total features: {len(feature_columns)}\")\n",
        "print(f\"    Base features: 7\")\n",
        "print(f\"    Season features: {len(season_features)}\")\n",
        "\n",
        "# Target variable\n",
        "target_column = 'bias'\n",
        "\n",
        "# Prepare data\n",
        "df_clean = df[feature_columns + [target_column, 'city', 'category']].dropna()\n",
        "print(f\"\\n‚úì Clean dataset: {len(df_clean)} samples\")\n",
        "\n",
        "X = df_clean[feature_columns].values\n",
        "y = df_clean[target_column].values\n",
        "\n",
        "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
        "print(f\"Target vector shape: {y.shape}\")\n",
        "\n",
        "# Feature statistics\n",
        "print(f\"\\nFeature ranges:\")\n",
        "for i, col in enumerate(feature_columns[:7]):  # Skip one-hot encoded\n",
        "    print(f\"  {col:<25}: [{X[:, i].min():.3f}, {X[:, i].max():.3f}]\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 2: TRAIN-TEST SPLIT\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"STEP 2: TRAIN-TEST SPLIT\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Random 80-20 split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining set: {len(X_train)} samples ({len(X_train)/len(X)*100:.1f}%)\")\n",
        "print(f\"Test set: {len(X_test)} samples ({len(X_test)/len(X)*100:.1f}%)\")\n",
        "\n",
        "# Store indices for later\n",
        "train_idx = df_clean.index[:(len(X_train))]\n",
        "test_idx = df_clean.index[len(X_train):]\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 3: TRAIN MODELS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"STEP 3: TRAINING MODELS\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Model 1: Random Forest\n",
        "print(\"\\n1. Random Forest Regressor\")\n",
        "print(\"   Hyperparameters:\")\n",
        "rf_params = {\n",
        "    'n_estimators': 200,\n",
        "    'max_depth': 15,\n",
        "    'min_samples_split': 5,\n",
        "    'min_samples_leaf': 2,\n",
        "    'random_state': 42,\n",
        "    'n_jobs': -1\n",
        "}\n",
        "for param, value in rf_params.items():\n",
        "    print(f\"     {param}: {value}\")\n",
        "\n",
        "rf_model = RandomForestRegressor(**rf_params)\n",
        "rf_model.fit(X_train, y_train)\n",
        "print(\"   ‚úì Training complete\")\n",
        "\n",
        "# Model 2: Gradient Boosting\n",
        "print(\"\\n2. Gradient Boosting Regressor\")\n",
        "print(\"   Hyperparameters:\")\n",
        "gb_params = {\n",
        "    'n_estimators': 150,\n",
        "    'max_depth': 8,\n",
        "    'learning_rate': 0.1,\n",
        "    'subsample': 0.8,\n",
        "    'random_state': 42\n",
        "}\n",
        "for param, value in gb_params.items():\n",
        "    print(f\"     {param}: {value}\")\n",
        "\n",
        "gb_model = GradientBoostingRegressor(**gb_params)\n",
        "gb_model.fit(X_train, y_train)\n",
        "print(\"   ‚úì Training complete\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 4: EVALUATE ON TEST SET\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"STEP 4: TEST SET EVALUATION\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "models = {\n",
        "    'Random Forest': rf_model,\n",
        "    'Gradient Boosting': gb_model\n",
        "}\n",
        "\n",
        "test_results = []\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    print(f\"\\n{model_name}:\")\n",
        "\n",
        "    # Predictions\n",
        "    y_pred_train = model.predict(X_train)\n",
        "    y_pred_test = model.predict(X_test)\n",
        "\n",
        "    # Metrics - Training\n",
        "    train_r2 = r2_score(y_train, y_pred_train)\n",
        "    train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
        "    train_mae = mean_absolute_error(y_train, y_pred_train)\n",
        "\n",
        "    # Metrics - Test\n",
        "    test_r2 = r2_score(y_test, y_pred_test)\n",
        "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
        "    test_mae = mean_absolute_error(y_test, y_pred_test)\n",
        "\n",
        "    print(f\"  Training: R¬≤={train_r2:.4f}, RMSE={train_rmse:.3f}¬∞C, MAE={train_mae:.3f}¬∞C\")\n",
        "    print(f\"  Test:     R¬≤={test_r2:.4f}, RMSE={test_rmse:.3f}¬∞C, MAE={test_mae:.3f}¬∞C\")\n",
        "\n",
        "    # Check for overfitting\n",
        "    if train_r2 - test_r2 > 0.1:\n",
        "        print(f\"  ‚ö†Ô∏è  Warning: Possible overfitting (R¬≤ gap: {train_r2 - test_r2:.3f})\")\n",
        "    else:\n",
        "        print(f\"  ‚úì Good generalization\")\n",
        "\n",
        "    test_results.append({\n",
        "        'Model': model_name,\n",
        "        'Train_R2': train_r2,\n",
        "        'Train_RMSE': train_rmse,\n",
        "        'Train_MAE': train_mae,\n",
        "        'Test_R2': test_r2,\n",
        "        'Test_RMSE': test_rmse,\n",
        "        'Test_MAE': test_mae,\n",
        "        'Predictions_Train': y_pred_train,\n",
        "        'Predictions_Test': y_pred_test\n",
        "    })\n",
        "\n",
        "df_test_results = pd.DataFrame(test_results)\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 5: CROSS-VALIDATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"STEP 5: CROSS-VALIDATION (5-FOLD)\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "cv_results = []\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    print(f\"\\n{model_name}:\")\n",
        "\n",
        "    # 5-fold cross-validation\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    cv_r2 = cross_val_score(model, X, y, cv=kf, scoring='r2', n_jobs=-1)\n",
        "    cv_neg_mse = cross_val_score(model, X, y, cv=kf, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "    cv_rmse = np.sqrt(-cv_neg_mse)\n",
        "\n",
        "    print(f\"  R¬≤ scores: {cv_r2}\")\n",
        "    print(f\"  Mean R¬≤: {cv_r2.mean():.4f} (¬±{cv_r2.std():.4f})\")\n",
        "    print(f\"  RMSE scores: {cv_rmse}\")\n",
        "    print(f\"  Mean RMSE: {cv_rmse.mean():.3f}¬∞C (¬±{cv_rmse.std():.3f})\")\n",
        "\n",
        "    cv_results.append({\n",
        "        'Model': model_name,\n",
        "        'CV_R2_Mean': cv_r2.mean(),\n",
        "        'CV_R2_Std': cv_r2.std(),\n",
        "        'CV_RMSE_Mean': cv_rmse.mean(),\n",
        "        'CV_RMSE_Std': cv_rmse.std()\n",
        "    })\n",
        "\n",
        "df_cv_results = pd.DataFrame(cv_results)\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 6: SPATIAL CROSS-VALIDATION (LEAVE-ONE-CITY-OUT)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"STEP 6: SPATIAL CROSS-VALIDATION (LEAVE-ONE-CITY-OUT)\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "cities = df_clean['city'].unique()\n",
        "print(f\"\\nTesting generalization across {len(cities)} cities\")\n",
        "\n",
        "spatial_cv_results = []\n",
        "\n",
        "for test_city in cities:\n",
        "    print(f\"\\n  Held-out city: {test_city}\")\n",
        "\n",
        "    # Split by city\n",
        "    train_mask = df_clean['city'] != test_city\n",
        "    test_mask = df_clean['city'] == test_city\n",
        "\n",
        "    X_train_cv = X[train_mask]\n",
        "    y_train_cv = y[train_mask]\n",
        "    X_test_cv = X[test_mask]\n",
        "    y_test_cv = y[test_mask]\n",
        "\n",
        "    print(f\"    Train: {len(X_train_cv)} stations from {len(cities)-1} cities\")\n",
        "    print(f\"    Test:  {len(X_test_cv)} stations from {test_city}\")\n",
        "\n",
        "    for model_name, model in models.items():\n",
        "        # Train on other cities\n",
        "        model_cv = model.__class__(**model.get_params())\n",
        "        model_cv.fit(X_train_cv, y_train_cv)\n",
        "\n",
        "        # Predict on held-out city\n",
        "        y_pred_cv = model_cv.predict(X_test_cv)\n",
        "\n",
        "        # Metrics\n",
        "        r2_cv = r2_score(y_test_cv, y_pred_cv)\n",
        "        rmse_cv = np.sqrt(mean_squared_error(y_test_cv, y_pred_cv))\n",
        "\n",
        "        spatial_cv_results.append({\n",
        "            'Model': model_name,\n",
        "            'Held_Out_City': test_city,\n",
        "            'N_Test': len(y_test_cv),\n",
        "            'R2': r2_cv,\n",
        "            'RMSE': rmse_cv\n",
        "        })\n",
        "\n",
        "df_spatial_cv = pd.DataFrame(spatial_cv_results)\n",
        "\n",
        "print(\"\\n  Summary by model:\")\n",
        "for model_name in models.keys():\n",
        "    model_results = df_spatial_cv[df_spatial_cv['Model'] == model_name]\n",
        "    print(f\"\\n  {model_name}:\")\n",
        "    print(f\"    Mean R¬≤: {model_results['R2'].mean():.4f} (¬±{model_results['R2'].std():.4f})\")\n",
        "    print(f\"    Mean RMSE: {model_results['RMSE'].mean():.3f}¬∞C (¬±{model_results['RMSE'].std():.3f})\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 7: FEATURE IMPORTANCE\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"STEP 7: FEATURE IMPORTANCE ANALYSIS\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Use Random Forest for feature importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': feature_columns,\n",
        "    'Importance': rf_model.feature_importances_\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "print(\"\\nTop 10 Most Important Features:\")\n",
        "print(f\"{'Rank':<6} {'Feature':<25} {'Importance':<12} {'Relative %':<12}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "total_importance = feature_importance['Importance'].sum()\n",
        "for idx, row in feature_importance.head(10).iterrows():\n",
        "    rank = feature_importance.index.get_loc(idx) + 1\n",
        "    relative_pct = (row['Importance'] / total_importance) * 100\n",
        "    print(f\"{rank:<6} {row['Feature']:<25} {row['Importance']:>8.4f}    {relative_pct:>6.2f}%\")\n",
        "\n",
        "# Save feature importance\n",
        "feature_importance.to_csv(f\"{WEEK4_DIR}/model2_feature_importance.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "6ZdPtfWk4HQ6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d99400b-66c5-486b-ea02-149e3b9961d4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "WEEK 4 - MODEL 2: MACHINE LEARNING ERROR PREDICTOR\n",
            "================================================================================\n",
            "\n",
            "Loaded 286 stations from Week 3\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "STEP 1: FEATURE ENGINEERING\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Adding seasonal features...\n",
            "  ‚úì Total features: 11\n",
            "    Base features: 7\n",
            "    Season features: 4\n",
            "\n",
            "‚úì Clean dataset: 286 samples\n",
            "\n",
            "Feature matrix shape: (286, 11)\n",
            "Target vector shape: (286,)\n",
            "\n",
            "Feature ranges:\n",
            "  ndvi_mean                : [-0.258, 0.737]\n",
            "  elevation                : [1.000, 2472.000]\n",
            "  lat                      : [39.687, 53.746]\n",
            "  lon                      : [-5.498, 22.550]\n",
            "  distance_to_city_km      : [1.730, 148.967]\n",
            "  distance_to_coast_km     : [173.347, 1026.540]\n",
            "  mean_station_temp        : [3.883, 24.728]\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "STEP 2: TRAIN-TEST SPLIT\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Training set: 228 samples (79.7%)\n",
            "Test set: 58 samples (20.3%)\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "STEP 3: TRAINING MODELS\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "1. Random Forest Regressor\n",
            "   Hyperparameters:\n",
            "     n_estimators: 200\n",
            "     max_depth: 15\n",
            "     min_samples_split: 5\n",
            "     min_samples_leaf: 2\n",
            "     random_state: 42\n",
            "     n_jobs: -1\n",
            "   ‚úì Training complete\n",
            "\n",
            "2. Gradient Boosting Regressor\n",
            "   Hyperparameters:\n",
            "     n_estimators: 150\n",
            "     max_depth: 8\n",
            "     learning_rate: 0.1\n",
            "     subsample: 0.8\n",
            "     random_state: 42\n",
            "   ‚úì Training complete\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "STEP 4: TEST SET EVALUATION\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Random Forest:\n",
            "  Training: R¬≤=0.7995, RMSE=0.512¬∞C, MAE=0.274¬∞C\n",
            "  Test:     R¬≤=0.2870, RMSE=1.029¬∞C, MAE=0.622¬∞C\n",
            "  ‚ö†Ô∏è  Warning: Possible overfitting (R¬≤ gap: 0.513)\n",
            "\n",
            "Gradient Boosting:\n",
            "  Training: R¬≤=1.0000, RMSE=0.001¬∞C, MAE=0.001¬∞C\n",
            "  Test:     R¬≤=0.5369, RMSE=0.829¬∞C, MAE=0.538¬∞C\n",
            "  ‚ö†Ô∏è  Warning: Possible overfitting (R¬≤ gap: 0.463)\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "STEP 5: CROSS-VALIDATION (5-FOLD)\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Random Forest:\n",
            "  R¬≤ scores: [0.30683091 0.22219393 0.19839462 0.27301513 0.16905091]\n",
            "  Mean R¬≤: 0.2339 (¬±0.0499)\n",
            "  RMSE scores: [1.01464497 0.89103884 0.87340515 0.92550428 1.28891048]\n",
            "  Mean RMSE: 0.999¬∞C (¬±0.153)\n",
            "\n",
            "Gradient Boosting:\n",
            "  R¬≤ scores: [ 0.48415738  0.19527043  0.58933786  0.27699278 -0.01254584]\n",
            "  Mean R¬≤: 0.3066 (¬±0.2129)\n",
            "  RMSE scores: [0.87529234 0.90632915 0.62514069 0.92296888 1.42279684]\n",
            "  Mean RMSE: 0.951¬∞C (¬±0.260)\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "STEP 6: SPATIAL CROSS-VALIDATION (LEAVE-ONE-CITY-OUT)\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Testing generalization across 5 cities\n",
            "\n",
            "  Held-out city: Berlin\n",
            "    Train: 200 stations from 4 cities\n",
            "    Test:  86 stations from Berlin\n",
            "\n",
            "  Held-out city: Paris\n",
            "    Train: 281 stations from 4 cities\n",
            "    Test:  5 stations from Paris\n",
            "\n",
            "  Held-out city: Milano\n",
            "    Train: 198 stations from 4 cities\n",
            "    Test:  88 stations from Milano\n",
            "\n",
            "  Held-out city: Warszawa\n",
            "    Train: 266 stations from 4 cities\n",
            "    Test:  20 stations from Warszawa\n",
            "\n",
            "  Held-out city: Madrid\n",
            "    Train: 199 stations from 4 cities\n",
            "    Test:  87 stations from Madrid\n",
            "\n",
            "  Summary by model:\n",
            "\n",
            "  Random Forest:\n",
            "    Mean R¬≤: -13.4901 (¬±28.8633)\n",
            "    Mean RMSE: 1.270¬∞C (¬±0.892)\n",
            "\n",
            "  Gradient Boosting:\n",
            "    Mean R¬≤: -24.2665 (¬±51.6670)\n",
            "    Mean RMSE: 1.474¬∞C (¬±1.113)\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "STEP 7: FEATURE IMPORTANCE ANALYSIS\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Top 10 Most Important Features:\n",
            "Rank   Feature                   Importance   Relative %  \n",
            "------------------------------------------------------------\n",
            "1      elevation                   0.3324     33.24%\n",
            "2      mean_station_temp           0.2092     20.92%\n",
            "3      lat                         0.1329     13.29%\n",
            "4      distance_to_coast_km        0.1010     10.10%\n",
            "5      distance_to_city_km         0.0750      7.50%\n",
            "6      lon                         0.0680      6.80%\n",
            "7      ndvi_mean                   0.0628      6.28%\n",
            "8      season_Spring               0.0070      0.70%\n",
            "9      season_Fall                 0.0055      0.55%\n",
            "10     season_Summer               0.0033      0.33%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NaxWWz7W-7NL",
        "outputId": "ad480c4f-0c8b-4d3d-dedc-afbb7d87f87b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['ndvi_mean', 'elevation', 'lat', 'lon', 'distance_to_city_km',\n",
              "       'distance_to_coast_km', 'mean_station_temp', 'season_Fall',\n",
              "       'season_Spring', 'season_Summer', 'season_Winter', 'bias', 'city',\n",
              "       'category'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================================================\n",
        "# STEP 8: APPLY CORRECTIONS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"STEP 8: APPLYING ML CORRECTIONS\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Use best model (Random Forest)\n",
        "best_model = rf_model\n",
        "best_model_name = 'Random Forest'\n",
        "\n",
        "print(f\"\\nUsing {best_model_name} for corrections\")\n",
        "\n",
        "# Predict errors for all data\n",
        "y_pred_all = best_model.predict(X)\n",
        "\n",
        "# Apply corrections to ERA5\n",
        "df_clean['predicted_error'] = y_pred_all\n",
        "df_clean['era5_corrected_ml'] = df_clean['mean_station_temp'] - df_clean['predicted_error']\n",
        "df_clean['error_ml'] = df_clean['era5_corrected_ml'] - df_clean['mean_station_temp']\n",
        "\n",
        "# Calculate improvement\n",
        "original_rmse = df['rmse'].mean()\n",
        "ml_rmse = np.sqrt((df_clean['error_ml']**2).mean())\n",
        "improvement = ((original_rmse - ml_rmse) / original_rmse) * 100\n",
        "\n",
        "print(f\"\\nOriginal ERA5 RMSE: {original_rmse:.3f}¬∞C\")\n",
        "print(f\"ML-Corrected RMSE: {ml_rmse:.3f}¬∞C\")\n",
        "print(f\"Improvement: {improvement:+.1f}%\")\n",
        "\n",
        "# Save corrected dataset\n",
        "df_clean.to_csv(f\"{WEEK4_DIR}/model2_corrected_dataset.csv\", index=False)\n",
        "\n",
        "# ============================================================================\n",
        "# VISUALIZATIONS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"STEP 9: CREATING VISUALIZATIONS\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Viz 1: Model Comparison - Performance Metrics\n",
        "print(\"\\nCreating Viz 1: Model Performance Comparison...\")\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "metrics = ['Train_R2', 'Test_R2', 'CV_R2_Mean']\n",
        "titles = ['Training R¬≤', 'Test R¬≤', 'Cross-Validation R¬≤']\n",
        "\n",
        "for ax, metric, title in zip(axes, metrics, titles):\n",
        "    if metric in df_test_results.columns:\n",
        "        values = df_test_results[metric].values\n",
        "    else:\n",
        "        values = df_cv_results[metric].values\n",
        "\n",
        "    bars = ax.bar(df_test_results['Model'], values,\n",
        "                  color=['#3498db', '#e74c3c'], alpha=0.7, edgecolor='black', linewidth=1.5)\n",
        "\n",
        "    for bar, val in zip(bars, values):\n",
        "        height = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "                f'{val:.4f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
        "\n",
        "    ax.set_ylabel('R¬≤ Score', fontsize=12)\n",
        "    ax.set_title(title, fontsize=13, fontweight='bold')\n",
        "    ax.set_ylim([0, max(values) * 1.15])\n",
        "    ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "fig.suptitle('Model 2: Machine Learning Performance Comparison',\n",
        "             fontsize=15, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{WEEK4_DIR}/viz1_model2_performance_comparison.png\", dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"  ‚úì Saved: viz1_model2_performance_comparison.png\")\n",
        "\n",
        "# Viz 2: Predicted vs Actual (Train and Test)\n",
        "print(\"\\nCreating Viz 2: Predicted vs Actual Errors...\")\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
        "\n",
        "rf_result = df_test_results[df_test_results['Model'] == 'Random Forest'].iloc[0]\n",
        "\n",
        "# Training set\n",
        "ax = axes[0]\n",
        "scatter = ax.scatter(y_train, rf_result['Predictions_Train'],\n",
        "                    c=X_train[:, feature_columns.index('elevation')],\n",
        "                    cmap='terrain', s=50, alpha=0.6, edgecolors='black', linewidth=0.5)\n",
        "\n",
        "min_val = min(y_train.min(), rf_result['Predictions_Train'].min())\n",
        "max_val = max(y_train.max(), rf_result['Predictions_Train'].max())\n",
        "ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2,\n",
        "        label='Perfect Prediction', alpha=0.7)\n",
        "\n",
        "ax.set_xlabel('Actual ERA5 Bias (¬∞C)', fontsize=12)\n",
        "ax.set_ylabel('Predicted ERA5 Bias (¬∞C)', fontsize=12)\n",
        "ax.set_title(f'Training Set (n={len(y_train)})\\nR¬≤={rf_result[\"Train_R2\"]:.4f}, RMSE={rf_result[\"Train_RMSE\"]:.3f}¬∞C',\n",
        "             fontsize=13, fontweight='bold')\n",
        "ax.legend(fontsize=10)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Test set\n",
        "ax = axes[1]\n",
        "scatter = ax.scatter(y_test, rf_result['Predictions_Test'],\n",
        "                    c=X_test[:, feature_columns.index('elevation')],\n",
        "                    cmap='terrain', s=50, alpha=0.6, edgecolors='black', linewidth=0.5)\n",
        "\n",
        "min_val = min(y_test.min(), rf_result['Predictions_Test'].min())\n",
        "max_val = max(y_test.max(), rf_result['Predictions_Test'].max())\n",
        "ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2,\n",
        "        label='Perfect Prediction', alpha=0.7)\n",
        "\n",
        "ax.set_xlabel('Actual ERA5 Bias (¬∞C)', fontsize=12)\n",
        "ax.set_ylabel('Predicted ERA5 Bias (¬∞C)', fontsize=12)\n",
        "ax.set_title(f'Test Set (n={len(y_test)})\\nR¬≤={rf_result[\"Test_R2\"]:.4f}, RMSE={rf_result[\"Test_RMSE\"]:.3f}¬∞C',\n",
        "             fontsize=13, fontweight='bold')\n",
        "ax.legend(fontsize=10)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "fig.colorbar(scatter, ax=axes, label='Elevation (m)', orientation='horizontal', pad=0.1)\n",
        "\n",
        "fig.suptitle('Model 2: Random Forest Prediction Accuracy',\n",
        "             fontsize=15, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{WEEK4_DIR}/viz2_model2_predictions.png\", dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"  ‚úì Saved: viz2_model2_predictions.png\")\n",
        "\n",
        "# Viz 3: Feature Importance\n",
        "print(\"\\nCreating Viz 3: Feature Importance...\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "top_features = feature_importance.head(10)\n",
        "colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(top_features)))\n",
        "\n",
        "bars = ax.barh(range(len(top_features)), top_features['Importance'],\n",
        "               color=colors, edgecolor='black', linewidth=1.5)\n",
        "\n",
        "ax.set_yticks(range(len(top_features)))\n",
        "ax.set_yticklabels(top_features['Feature'])\n",
        "ax.invert_yaxis()\n",
        "ax.set_xlabel('Importance Score', fontsize=13)\n",
        "ax.set_title('Model 2: Top 10 Feature Importances (Random Forest)',\n",
        "             fontsize=14, fontweight='bold')\n",
        "ax.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# Add values\n",
        "for i, (bar, imp) in enumerate(zip(bars, top_features['Importance'])):\n",
        "    width = bar.get_width()\n",
        "    relative_pct = (imp / total_importance) * 100\n",
        "    ax.text(width + 0.005, bar.get_y() + bar.get_height()/2.,\n",
        "            f'{imp:.4f} ({relative_pct:.1f}%)',\n",
        "            va='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{WEEK4_DIR}/viz3_model2_feature_importance.png\", dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"  ‚úì Saved: viz3_model2_feature_importance.png\")\n",
        "\n",
        "# Viz 4: Spatial Cross-Validation Results\n",
        "print(\"\\nCreating Viz 4: Spatial Cross-Validation...\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 7))\n",
        "\n",
        "rf_spatial = df_spatial_cv[df_spatial_cv['Model'] == 'Random Forest']\n",
        "x_pos = np.arange(len(rf_spatial))\n",
        "\n",
        "bars = ax.bar(x_pos, rf_spatial['RMSE'],\n",
        "              color=['#27ae60' if r < original_rmse else '#e74c3c'\n",
        "                     for r in rf_spatial['RMSE']],\n",
        "              alpha=0.7, edgecolor='black', linewidth=1.5)\n",
        "\n",
        "# Add original RMSE line\n",
        "ax.axhline(original_rmse, color='red', linestyle='--', linewidth=2,\n",
        "          label=f'Original ERA5 RMSE: {original_rmse:.3f}¬∞C', alpha=0.7)\n",
        "\n",
        "# Add values\n",
        "for bar, rmse, r2 in zip(bars, rf_spatial['RMSE'], rf_spatial['R2']):\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height + 0.05,\n",
        "            f'{rmse:.3f}¬∞C\\nR¬≤={r2:.3f}',\n",
        "            ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
        "\n",
        "ax.set_xticks(x_pos)\n",
        "ax.set_xticklabels(rf_spatial['Held_Out_City'], rotation=0)\n",
        "ax.set_ylabel('RMSE (¬∞C)', fontsize=13)\n",
        "ax.set_xlabel('Held-Out City', fontsize=13)\n",
        "ax.set_title('Model 2: Leave-One-City-Out Cross-Validation\\nRandom Forest Performance',\n",
        "             fontsize=14, fontweight='bold')\n",
        "ax.legend(fontsize=11)\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{WEEK4_DIR}/viz4_model2_spatial_cv.png\", dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"  ‚úì Saved: viz4_model2_spatial_cv.png\")\n",
        "\n",
        "# Viz 5: Before vs After Correction\n",
        "print(\"\\nCreating Viz 5: Before/After ML Correction...\")\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
        "\n",
        "# Original\n",
        "ax = axes[0]\n",
        "scatter = ax.scatter(df_clean['mean_station_temp'], df_clean['mean_station_temp'],\n",
        "                    c=df_clean['elevation'], cmap='terrain', s=50, alpha=0.6,\n",
        "                    edgecolors='black', linewidth=0.5)\n",
        "\n",
        "min_val = min(df_clean['mean_station_temp'].min(), df_clean['mean_station_temp'].min())\n",
        "max_val = max(df_clean['mean_station_temp'].max(), df_clean['mean_station_temp'].max())\n",
        "ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2,\n",
        "        label='Perfect Agreement', alpha=0.7)\n",
        "\n",
        "ax.set_xlabel('Observed Temperature (¬∞C)', fontsize=12)\n",
        "ax.set_ylabel('ERA5 Temperature (¬∞C)', fontsize=12)\n",
        "ax.set_title(f'Original ERA5\\nRMSE: {original_rmse:.3f}¬∞C',\n",
        "             fontsize=13, fontweight='bold')\n",
        "ax.legend(fontsize=10)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# ML Corrected\n",
        "ax = axes[1]\n",
        "scatter = ax.scatter(df_clean['mean_station_temp'], df_clean['era5_corrected_ml'],\n",
        "                    c=df_clean['elevation'], cmap='terrain', s=50, alpha=0.6,\n",
        "                    edgecolors='black', linewidth=0.5)\n",
        "\n",
        "min_val = min(df_clean['mean_station_temp'].min(), df_clean['era5_corrected_ml'].min())\n",
        "max_val = max(df_clean['mean_station_temp'].max(), df_clean['era5_corrected_ml'].max())\n",
        "ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2,\n",
        "        label='Perfect Agreement', alpha=0.7)\n",
        "\n",
        "ax.set_xlabel('Observed Temperature (¬∞C)', fontsize=12)\n",
        "ax.set_ylabel('Corrected ERA5 Temperature (¬∞C)', fontsize=12)\n",
        "ax.set_title(f'ML-Corrected ERA5\\nRMSE: {ml_rmse:.3f}¬∞C ({improvement:+.1f}%)',\n",
        "             fontsize=13, fontweight='bold')\n",
        "ax.legend(fontsize=10)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "fig.colorbar(scatter, ax=axes, label='Elevation (m)', orientation='horizontal', pad=0.1)\n",
        "\n",
        "fig.suptitle('Model 2: Impact of Machine Learning Correction',\n",
        "             fontsize=15, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{WEEK4_DIR}/viz5_model2_correction_impact.png\", dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"  ‚úì Saved: viz5_model2_correction_impact.png\")\n",
        "\n",
        "# Viz 6: Residuals Analysis\n",
        "print(\"\\nCreating Viz 6: Residuals Analysis...\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "rf_residuals_train = y_train - rf_result['Predictions_Train']\n",
        "rf_residuals_test = y_test - rf_result['Predictions_Test']\n",
        "\n",
        "# Training residuals vs predicted\n",
        "ax = axes[0, 0]\n",
        "ax.scatter(rf_result['Predictions_Train'], rf_residuals_train,\n",
        "          alpha=0.5, s=30, c='steelblue', edgecolors='black', linewidth=0.5)\n",
        "ax.axhline(0, color='red', linestyle='--', linewidth=2)\n",
        "ax.set_xlabel('Predicted Bias (¬∞C)', fontsize=11)\n",
        "ax.set_ylabel('Residuals (¬∞C)', fontsize=11)\n",
        "ax.set_title('A) Training Residuals vs Predicted', fontsize=12, fontweight='bold')\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Test residuals vs predicted\n",
        "ax = axes[0, 1]\n",
        "ax.scatter(rf_result['Predictions_Test'], rf_residuals_test,\n",
        "          alpha=0.5, s=30, c='coral', edgecolors='black', linewidth=0.5)\n",
        "ax.axhline(0, color='red', linestyle='--', linewidth=2)\n",
        "ax.set_xlabel('Predicted Bias (¬∞C)', fontsize=11)\n",
        "ax.set_ylabel('Residuals (¬∞C)', fontsize=11)\n",
        "ax.set_title('B) Test Residuals vs Predicted', fontsize=12, fontweight='bold')\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Training residuals histogram\n",
        "ax = axes[1, 0]\n",
        "ax.hist(rf_residuals_train, bins=40, alpha=0.7, color='steelblue', edgecolor='black')\n",
        "ax.axvline(0, color='red', linestyle='--', linewidth=2)\n",
        "ax.set_xlabel('Residuals (¬∞C)', fontsize=11)\n",
        "ax.set_ylabel('Frequency', fontsize=11)\n",
        "ax.set_title(f'C) Training Residuals Distribution\\nMean: {rf_residuals_train.mean():+.4f}¬∞C, Std: {rf_residuals_train.std():.4f}¬∞C',\n",
        "             fontsize=12, fontweight='bold')\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Test residuals histogram\n",
        "ax = axes[1, 1]\n",
        "ax.hist(rf_residuals_test, bins=40, alpha=0.7, color='coral', edgecolor='black')\n",
        "ax.axvline(0, color='red', linestyle='--', linewidth=2)\n",
        "ax.set_xlabel('Residuals (¬∞C)', fontsize=11)\n",
        "ax.set_ylabel('Frequency', fontsize=11)\n",
        "ax.set_title(f'D) Test Residuals Distribution\\nMean: {rf_residuals_test.mean():+.4f}¬∞C, Std: {rf_residuals_test.std():.4f}¬∞C',\n",
        "             fontsize=12, fontweight='bold')\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "fig.suptitle('Model 2: Residuals Analysis (Random Forest)',\n",
        "             fontsize=15, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{WEEK4_DIR}/viz6_model2_residuals.png\", dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"  ‚úì Saved: viz6_model2_residuals.png\")\n",
        "\n",
        "# ============================================================================\n",
        "# SAVE ALL RESULTS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"SAVING RESULTS\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Save test results\n",
        "df_test_results[['Model', 'Train_R2', 'Train_RMSE', 'Train_MAE',\n",
        "                 'Test_R2', 'Test_RMSE', 'Test_MAE']].to_csv(\n",
        "    f\"{WEEK4_DIR}/model2_test_results.csv\", index=False\n",
        ")\n",
        "print(\"  ‚úì Test results: model2_test_results.csv\")\n",
        "\n",
        "# Save CV results\n",
        "df_cv_results.to_csv(f\"{WEEK4_DIR}/model2_cv_results.csv\", index=False)\n",
        "print(\"  ‚úì CV results: model2_cv_results.csv\")\n",
        "\n",
        "# Save spatial CV results\n",
        "df_spatial_cv.to_csv(f\"{WEEK4_DIR}/model2_spatial_cv_results.csv\", index=False)\n",
        "print(\"  ‚úì Spatial CV results: model2_spatial_cv_results.csv\")\n",
        "\n",
        "# ============================================================================\n",
        "# FINAL SUMMARY\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODEL 2: MACHINE LEARNING ERROR PREDICTOR - COMPLETE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nüìä BEST MODEL: Random Forest\")\n",
        "print(f\"\\n  Training Performance:\")\n",
        "print(f\"    R¬≤:   {rf_result['Train_R2']:.4f}\")\n",
        "print(f\"    RMSE: {rf_result['Train_RMSE']:.3f}¬∞C\")\n",
        "print(f\"    MAE:  {rf_result['Train_MAE']:.3f}¬∞C\")\n",
        "\n",
        "print(f\"\\n  Test Performance:\")\n",
        "print(f\"    R¬≤:   {rf_result['Test_R2']:.4f}\")\n",
        "print(f\"    RMSE: {rf_result['Test_RMSE']:.3f}¬∞C\")\n",
        "print(f\"    MAE:  {rf_result['Test_MAE']:.3f}¬∞C\")\n",
        "\n",
        "print(f\"\\n  Cross-Validation:\")\n",
        "rf_cv = df_cv_results[df_cv_results['Model'] == 'Random Forest'].iloc[0]\n",
        "print(f\"    R¬≤:   {rf_cv['CV_R2_Mean']:.4f} (¬±{rf_cv['CV_R2_Std']:.4f})\")\n",
        "print(f\"    RMSE: {rf_cv['CV_RMSE_Mean']:.3f}¬∞C (¬±{rf_cv['CV_RMSE_Std']:.3f})\")\n",
        "\n",
        "print(f\"\\n  Spatial Cross-Validation:\")\n",
        "rf_spatial_mean = df_spatial_cv[df_spatial_cv['Model'] == 'Random Forest']\n",
        "print(f\"    Mean R¬≤:   {rf_spatial_mean['R2'].mean():.4f} (¬±{rf_spatial_mean['R2'].std():.4f})\")\n",
        "print(f\"    Mean RMSE: {rf_spatial_mean['RMSE'].mean():.3f}¬∞C (¬±{rf_spatial_mean['RMSE'].std():.3f})\")\n",
        "\n",
        "print(f\"\\nüéØ CORRECTION PERFORMANCE:\")\n",
        "print(f\"  Original ERA5 RMSE: {original_rmse:.3f}¬∞C\")\n",
        "print(f\"  ML-Corrected RMSE:  {ml_rmse:.3f}¬∞C\")\n",
        "print(f\"  Improvement:        {improvement:+.1f}%\")\n",
        "\n",
        "print(f\"\\nüèÜ TOP 3 FEATURES:\")\n",
        "for i, row in feature_importance.head(3).iterrows():\n",
        "    pct = (row['Importance'] / total_importance) * 100\n",
        "    print(f\"  {i+1}. {row['Feature']:<25} ({pct:.1f}%)\")\n",
        "\n",
        "print(\"\\nüìÅ OUTPUTS SAVED:\")\n",
        "print(\"  1. model2_feature_importance.csv - Feature rankings\")\n",
        "print(\"  2. model2_corrected_dataset.csv - Full corrected dataset\")\n",
        "print(\"  3. model2_test_results.csv - Test set performance\")\n",
        "print(\"  4. model2_cv_results.csv - Cross-validation results\")\n",
        "print(\"  5. model2_spatial_cv_results.csv - Spatial CV results\")\n",
        "print(\"  6. viz1_model2_performance_comparison.png - Model comparison\")\n",
        "print(\"  7. viz2_model2_predictions.png - Predicted vs actual\")\n",
        "print(\"  8. viz3_model2_feature_importance.png - Feature importance chart\")\n",
        "print(\"  9. viz4_model2_spatial_cv.png - Spatial generalization\")\n",
        "print(\" 10. viz5_model2_correction_impact.png - Before/after correction\")\n",
        "print(\" 11. viz6_model2_residuals.png - Residuals analysis\")\n",
        "\n",
        "print(\"\\n‚úì Model 2 analysis complete!\")\n",
        "print(\"\\n\" + \"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkVe3VXf9Bvy",
        "outputId": "d7ccabab-1b0a-457e-c32a-9e0856725cf1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------------------------------------\n",
            "STEP 8: APPLYING ML CORRECTIONS\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Using Random Forest for corrections\n",
            "\n",
            "Original ERA5 RMSE: 2.048¬∞C\n",
            "ML-Corrected RMSE: 1.459¬∞C\n",
            "Improvement: +28.7%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "STEP 9: CREATING VISUALIZATIONS\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Creating Viz 1: Model Performance Comparison...\n",
            "  ‚úì Saved: viz1_model2_performance_comparison.png\n",
            "\n",
            "Creating Viz 2: Predicted vs Actual Errors...\n",
            "  ‚úì Saved: viz2_model2_predictions.png\n",
            "\n",
            "Creating Viz 3: Feature Importance...\n",
            "  ‚úì Saved: viz3_model2_feature_importance.png\n",
            "\n",
            "Creating Viz 4: Spatial Cross-Validation...\n",
            "  ‚úì Saved: viz4_model2_spatial_cv.png\n",
            "\n",
            "Creating Viz 5: Before/After ML Correction...\n",
            "  ‚úì Saved: viz5_model2_correction_impact.png\n",
            "\n",
            "Creating Viz 6: Residuals Analysis...\n",
            "  ‚úì Saved: viz6_model2_residuals.png\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "SAVING RESULTS\n",
            "--------------------------------------------------------------------------------\n",
            "  ‚úì Test results: model2_test_results.csv\n",
            "  ‚úì CV results: model2_cv_results.csv\n",
            "  ‚úì Spatial CV results: model2_spatial_cv_results.csv\n",
            "\n",
            "================================================================================\n",
            "MODEL 2: MACHINE LEARNING ERROR PREDICTOR - COMPLETE\n",
            "================================================================================\n",
            "\n",
            "üìä BEST MODEL: Random Forest\n",
            "\n",
            "  Training Performance:\n",
            "    R¬≤:   0.7995\n",
            "    RMSE: 0.512¬∞C\n",
            "    MAE:  0.274¬∞C\n",
            "\n",
            "  Test Performance:\n",
            "    R¬≤:   0.2870\n",
            "    RMSE: 1.029¬∞C\n",
            "    MAE:  0.622¬∞C\n",
            "\n",
            "  Cross-Validation:\n",
            "    R¬≤:   0.2339 (¬±0.0499)\n",
            "    RMSE: 0.999¬∞C (¬±0.153)\n",
            "\n",
            "  Spatial Cross-Validation:\n",
            "    Mean R¬≤:   -13.4901 (¬±28.8633)\n",
            "    Mean RMSE: 1.270¬∞C (¬±0.892)\n",
            "\n",
            "üéØ CORRECTION PERFORMANCE:\n",
            "  Original ERA5 RMSE: 2.048¬∞C\n",
            "  ML-Corrected RMSE:  1.459¬∞C\n",
            "  Improvement:        +28.7%\n",
            "\n",
            "üèÜ TOP 3 FEATURES:\n",
            "  2. elevation                 (33.2%)\n",
            "  7. mean_station_temp         (20.9%)\n",
            "  3. lat                       (13.3%)\n",
            "\n",
            "üìÅ OUTPUTS SAVED:\n",
            "  1. model2_feature_importance.csv - Feature rankings\n",
            "  2. model2_corrected_dataset.csv - Full corrected dataset\n",
            "  3. model2_test_results.csv - Test set performance\n",
            "  4. model2_cv_results.csv - Cross-validation results\n",
            "  5. model2_spatial_cv_results.csv - Spatial CV results\n",
            "  6. viz1_model2_performance_comparison.png - Model comparison\n",
            "  7. viz2_model2_predictions.png - Predicted vs actual\n",
            "  8. viz3_model2_feature_importance.png - Feature importance chart\n",
            "  9. viz4_model2_spatial_cv.png - Spatial generalization\n",
            " 10. viz5_model2_correction_impact.png - Before/after correction\n",
            " 11. viz6_model2_residuals.png - Residuals analysis\n",
            "\n",
            "‚úì Model 2 analysis complete!\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 3: Rigorous statistical approach with interpretable coefficients:\n",
        "  - Linear Mixed Effects Model (city as random effect)\n",
        "  - Multiple Linear Regression (baseline)\n",
        "  - Quantile Regression (robust to outliers)\n",
        "  - Statistical inference (p-values, confidence intervals)\n",
        "  - Model diagnostics (assumptions testing)\n",
        "  \n",
        "Goal: Scientifically rigorous, interpretable, publishable model\n"
      ],
      "metadata": {
        "id": "hK-tJWNNhJSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Statistical modeling libraries\n",
        "try:\n",
        "    import statsmodels.api as sm\n",
        "    import statsmodels.formula.api as smf\n",
        "    from statsmodels.regression.mixed_linear_model import MixedLM\n",
        "    statsmodels_available = True\n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è  Warning: statsmodels not available. Installing...\")\n",
        "    import subprocess\n",
        "    subprocess.check_call(['pip', 'install', 'statsmodels', '-q'])\n",
        "    import statsmodels.api as sm\n",
        "    import statsmodels.formula.api as smf\n",
        "    from statsmodels.regression.mixed_linear_model import MixedLM\n",
        "    statsmodels_available = True\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.dpi'] = 150\n",
        "plt.rcParams['savefig.dpi'] = 300\n",
        "plt.rcParams['font.size'] = 11\n",
        "\n",
        "# ============================================================================\n",
        "# LOAD DATA\n",
        "# ============================================================================\n",
        "\n",
        "WEEK3_DIR =\"/content/drive/MyDrive/GenHack2025/week3_quantitative_metrics\"\n",
        "WEEK4_DIR = \"week4_explanatory_model3\"\n",
        "os.makedirs(WEEK4_DIR, exist_ok=True)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"WEEK 4 - MODEL 3: STATISTICAL REGRESSION MODEL\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Load master dataset\n",
        "df = pd.read_csv(f\"{WEEK3_DIR}/master_dataset.csv\")\n",
        "print(f\"\\nLoaded {len(df)} stations from Week 3\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 1: DATA PREPARATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"STEP 1: DATA PREPARATION FOR REGRESSION\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Get dominant season\n",
        "def get_dominant_season(row):\n",
        "    seasons = []\n",
        "    for season in ['Winter', 'Spring', 'Summer', 'Fall']:\n",
        "        if f'n_days_{season}' in row.index and row[f'n_days_{season}'] > 0:\n",
        "            seasons.append((season, row[f'n_days_{season}']))\n",
        "    if seasons:\n",
        "        return max(seasons, key=lambda x: x[1])[0]\n",
        "    return 'Summer'\n",
        "\n",
        "df['season'] = df.apply(get_dominant_season, axis=1)\n",
        "\n",
        "# Prepare regression dataset\n",
        "regression_vars = [\n",
        "    'bias',  # Target\n",
        "    'ndvi_mean',\n",
        "    'elevation',\n",
        "    'lat',\n",
        "    'lon',\n",
        "    'distance_to_city_km',\n",
        "    'distance_to_coast_km',\n",
        "    'mean_station_temp',\n",
        "    'mean_era5_temp', # ADDED THIS LINE\n",
        "    'season',\n",
        "    'city',\n",
        "    'category'\n",
        "]\n",
        "\n",
        "df_reg = df[regression_vars].dropna()\n",
        "print(f\"\\n‚úì Clean regression dataset: {len(df_reg)} observations\")\n",
        "\n",
        "# Standardize continuous predictors for interpretation\n",
        "continuous_vars = ['elevation', 'lat', 'lon', 'distance_to_city_km',\n",
        "                   'distance_to_coast_km', 'mean_station_temp', 'ndvi_mean']\n",
        "\n",
        "print(\"\\nStandardizing continuous variables for interpretation...\")\n",
        "for var in continuous_vars:\n",
        "    mean_val = df_reg[var].mean()\n",
        "    std_val = df_reg[var].std()\n",
        "    df_reg[f'{var}_std'] = (df_reg[var] - mean_val) / std_val\n",
        "    print(f\"  {var:<25}: mean={mean_val:.2f}, std={std_val:.2f}\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 2: MODEL 1 - SIMPLE LINEAR REGRESSION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"STEP 2: SIMPLE LINEAR REGRESSION (BASELINE)\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Prepare design matrix\n",
        "X_simple = df_reg[['elevation_std', 'mean_station_temp_std', 'lat_std',\n",
        "                    'lon_std', 'distance_to_city_km_std', 'distance_to_coast_km_std',\n",
        "                    'ndvi_mean_std']].copy()\n",
        "\n",
        "# Add season dummies\n",
        "season_dummies = pd.get_dummies(df_reg['season'], prefix='season', drop_first=True)\n",
        "# Convert boolean dummies to integer type (0 or 1) to avoid ValueError in statsmodels\n",
        "season_dummies = season_dummies.astype(int)\n",
        "X_simple = pd.concat([X_simple, season_dummies], axis=1)\n",
        "\n",
        "# Add constant\n",
        "X_simple = sm.add_constant(X_simple)\n",
        "\n",
        "y = df_reg['bias']\n",
        "\n",
        "# Fit OLS model\n",
        "print(\"\\nFitting Ordinary Least Squares model...\")\n",
        "ols_model = sm.OLS(y, X_simple).fit()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MODEL 1: SIMPLE LINEAR REGRESSION RESULTS\")\n",
        "print(\"=\"*60)\n",
        "print(ols_model.summary())\n",
        "\n",
        "# Save summary\n",
        "with open(f\"{WEEK4_DIR}/model3_ols_summary.txt\", 'w') as f:\n",
        "    f.write(str(ols_model.summary()))\n",
        "print(f\"\\n‚úì Saved OLS summary: model3_ols_summary.txt\")\n",
        "\n",
        "# Extract key statistics\n",
        "ols_results = {\n",
        "    'Model': 'OLS',\n",
        "    'R2': ols_model.rsquared,\n",
        "    'Adj_R2': ols_model.rsquared_adj,\n",
        "    'AIC': ols_model.aic,\n",
        "    'BIC': ols_model.bic,\n",
        "    'RMSE': np.sqrt(ols_model.mse_resid),\n",
        "    'N_obs': int(ols_model.nobs)\n",
        "}\n",
        "\n",
        "print(f\"\\nüìä Model Statistics:\")\n",
        "print(f\"  R¬≤: {ols_results['R2']:.4f}\")\n",
        "print(f\"  Adjusted R¬≤: {ols_results['Adj_R2']:.4f}\")\n",
        "print(f\"  RMSE: {ols_results['RMSE']:.3f}¬∞C\")\n",
        "print(f\"  AIC: {ols_results['AIC']:.2f}\")\n",
        "print(f\"  BIC: {ols_results['BIC']:.2f}\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 3: MODEL 2 - LINEAR MIXED EFFECTS MODEL\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"STEP 3: LINEAR MIXED EFFECTS MODEL (HIERARCHICAL)\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "print(\"\\nFitting Mixed Effects Model with city as random effect...\")\n",
        "print(\"  Fixed effects: elevation, temperature, lat, lon, distances, NDVI, season\")\n",
        "print(\"  Random effect: city (accounts for city-to-city variation)\")\n",
        "\n",
        "# Prepare formula\n",
        "formula = \"\"\"bias ~ elevation_std + mean_station_temp_std + lat_std + lon_std +\n",
        "             distance_to_city_km_std + distance_to_coast_km_std + ndvi_mean_std +\n",
        "             C(season)\"\"\"\n",
        "\n",
        "# Fit mixed model\n",
        "try:\n",
        "    mixed_model = smf.mixedlm(formula, df_reg, groups=df_reg[\"city\"]).fit()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"MODEL 2: LINEAR MIXED EFFECTS MODEL RESULTS\")\n",
        "    print(\"=\"*60)\n",
        "    print(mixed_model.summary())\n",
        "\n",
        "    # Save summary\n",
        "    with open(f\"{WEEK4_DIR}/model3_mixed_summary.txt\", 'w') as f:\n",
        "        f.write(str(mixed_model.summary()))\n",
        "    print(f\"\\n‚úì Saved Mixed Model summary: model3_mixed_summary.txt\")\n",
        "\n",
        "    mixed_results = {\n",
        "        'Model': 'Mixed Effects',\n",
        "        'AIC': mixed_model.aic,\n",
        "        'BIC': mixed_model.bic,\n",
        "        'Log_Likelihood': mixed_model.llf,\n",
        "        'N_obs': int(mixed_model.nobs)\n",
        "    }\n",
        "\n",
        "    print(f\"\\nüìä Model Statistics:\")\n",
        "    print(f\"  AIC: {mixed_results['AIC']:.2f}\")\n",
        "    print(f\"  BIC: {mixed_results['BIC']:.2f}\")\n",
        "    print(f\"  Log-Likelihood: {mixed_results['Log_Likelihood']:.2f}\")\n",
        "\n",
        "    # Random effects variance\n",
        "    print(f\"\\nüé≤ Random Effects:\")\n",
        "    print(f\"  City variance: {mixed_model.cov_re.iloc[0,0]:.4f}\")\n",
        "    print(f\"  Residual variance: {mixed_model.scale:.4f}\")\n",
        "\n",
        "    mixed_available = True\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ö†Ô∏è  Mixed model fitting failed: {e}\")\n",
        "    print(\"   Continuing with OLS results only...\")\n",
        "    mixed_available = False\n",
        "    mixed_model = None\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 4: MODEL 3 - QUANTILE REGRESSION (ROBUST)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"STEP 4: QUANTILE REGRESSION (ROBUST TO OUTLIERS)\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "print(\"\\nFitting Quantile Regression at median (50th percentile)...\")\n",
        "print(\"  (Robust alternative to OLS, less sensitive to outliers)\")\n",
        "\n",
        "try:\n",
        "    # Prepare data\n",
        "    X_quant = X_simple.copy()\n",
        "\n",
        "    # Fit quantile regression at median\n",
        "    qr_model = sm.QuantReg(y, X_quant).fit(q=0.5)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"MODEL 3: QUANTILE REGRESSION RESULTS (Median)\")\n",
        "    print(\"=\"*60)\n",
        "    print(qr_model.summary())\n",
        "\n",
        "    # Save summary\n",
        "    with open(f\"{WEEK4_DIR}/model3_quantreg_summary.txt\", 'w') as f:\n",
        "        f.write(str(qr_model.summary()))\n",
        "    print(f\"\\n‚úì Saved Quantile Regression summary: model3_quantreg_summary.txt\")\n",
        "\n",
        "    quantreg_available = True\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ö†Ô∏è  Quantile regression failed: {e}\")\n",
        "    quantreg_available = False\n",
        "    qr_model = None\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 5: COEFFICIENT COMPARISON & INTERPRETATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"STEP 5: COEFFICIENT COMPARISON & INTERPRETATION\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Extract coefficients\n",
        "def extract_coefficients(model, model_name):\n",
        "    coefs = []\n",
        "    for param in model.params.index:\n",
        "        if param != 'const' and not param.startswith('Group'):\n",
        "            coef_val = model.params[param]\n",
        "            std_err = model.bse[param]\n",
        "            p_val = model.pvalues[param]\n",
        "            ci_lower = model.conf_int().loc[param, 0]\n",
        "            ci_upper = model.conf_int().loc[param, 1]\n",
        "\n",
        "            # Significance stars\n",
        "            if p_val < 0.001:\n",
        "                sig = '***'\n",
        "            elif p_val < 0.01:\n",
        "                sig = '**'\n",
        "            elif p_val < 0.05:\n",
        "                sig = '*'\n",
        "            else:\n",
        "                sig = ''\n",
        "\n",
        "            coefs.append({\n",
        "                'Model': model_name,\n",
        "                'Variable': param,\n",
        "                'Coefficient': coef_val,\n",
        "                'Std_Error': std_err,\n",
        "                'P_Value': p_val,\n",
        "                'CI_Lower': ci_lower,\n",
        "                'CI_Upper': ci_upper,\n",
        "                'Significance': sig\n",
        "            })\n",
        "    return coefs\n",
        "\n",
        "# Collect coefficients from all models\n",
        "all_coefs = []\n",
        "all_coefs.extend(extract_coefficients(ols_model, 'OLS'))\n",
        "\n",
        "if mixed_available:\n",
        "    all_coefs.extend(extract_coefficients(mixed_model, 'Mixed Effects'))\n",
        "\n",
        "if quantreg_available:\n",
        "    all_coefs.extend(extract_coefficients(qr_model, 'Quantile Reg'))\n",
        "\n",
        "df_coefs = pd.DataFrame(all_coefs)\n",
        "\n",
        "# Save coefficients\n",
        "df_coefs.to_csv(f\"{WEEK4_DIR}/model3_coefficients_comparison.csv\", index=False)\n",
        "print(f\"\\n‚úì Saved coefficient comparison: model3_coefficients_comparison.csv\")\n",
        "\n",
        "# Print key coefficients\n",
        "print(\"\\nüìä KEY COEFFICIENTS (OLS Model):\")\n",
        "print(f\"{'Variable':<30} {'Coef':<10} {'Std Err':<10} {'P-value':<10} {'Sig':<5}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "key_vars = ['elevation_std', 'mean_station_temp_std', 'ndvi_mean_std',\n",
        "            'distance_to_city_km_std', 'lat_std']\n",
        "\n",
        "for var in key_vars:\n",
        "    if var in ols_model.params.index:\n",
        "        coef = ols_model.params[var]\n",
        "        se = ols_model.bse[var]\n",
        "        pval = ols_model.pvalues[var]\n",
        "\n",
        "        if pval < 0.001:\n",
        "            sig = '***'\n",
        "        elif pval < 0.01:\n",
        "            sig = '**'\n",
        "        elif pval < 0.05:\n",
        "            sig = '*'\n",
        "        else:\n",
        "            sig = ''\n",
        "\n",
        "        print(f\"   {var:<30} {coef:>+8.4f}  {se:>8.4f}  {pval:>8.6f}  {sig:<5}\")\n",
        "\n",
        "print(\"\\nSignificance codes: *** p<0.001, ** p<0.01, * p<0.05\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 6: MODEL DIAGNOSTICS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"STEP 6: MODEL DIAGNOSTICS\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Residuals\n",
        "residuals = ols_model.resid\n",
        "fitted = ols_model.fittedvalues\n",
        "\n",
        "# Normality test\n",
        "print(\"\\n1. Normality Test (Jarque-Bera):\")\n",
        "jb_stat, jb_pval = stats.jarque_bera(residuals)\n",
        "print(f\"   Test statistic: {jb_stat:.4f}\")\n",
        "print(f\"   P-value: {jb_pval:.6f}\")\n",
        "if jb_pval > 0.05:\n",
        "    print(\"   ‚úì Residuals appear normally distributed\")\n",
        "else:\n",
        "    print(\"   ‚ö†Ô∏è  Residuals may not be perfectly normal (but OLS is robust)\")\n",
        "\n",
        "# Homoscedasticity test (Breusch-Pagan)\n",
        "print(\"\\n2. Homoscedasticity Test (Breusch-Pagan):\")\n",
        "from statsmodels.stats.diagnostic import het_breuschpagan\n",
        "bp_stat, bp_pval, _, _ = het_breuschpagan(residuals, X_simple)\n",
        "print(f\"   Test statistic: {bp_stat:.4f}\")\n",
        "print(f\"   P-value: {bp_pval:.6f}\")\n",
        "if bp_pval > 0.05:\n",
        "    print(\"   ‚úì Homoscedasticity assumption satisfied\")\n",
        "else:\n",
        "    print(\"   ‚ö†Ô∏è  Some heteroscedasticity detected (common in real data)\")\n",
        "\n",
        "# Multicollinearity (VIF)\n",
        "print(\"\\n3. Multicollinearity Check (VIF):\")\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "print(f\"   {'Variable':<30} {'VIF':<10} {'Status':<20}\")\n",
        "print(\"   \" + \"-\" * 60)\n",
        "\n",
        "vif_data = []\n",
        "for i, col in enumerate(X_simple.columns):\n",
        "    if col != 'const':\n",
        "        vif = variance_inflation_factor(X_simple.values, i)\n",
        "        vif_data.append({'Variable': col, 'VIF': vif})\n",
        "\n",
        "        if vif < 5:\n",
        "            status = '‚úì Low'\n",
        "        elif vif < 10:\n",
        "            status = '‚ö†Ô∏è  Moderate'\n",
        "        else:\n",
        "            status = '‚ùå High'\n",
        "\n",
        "        print(f\"   {col:<30} {vif:>8.2f}  {status:<20}\")\n",
        "\n",
        "df_vif = pd.DataFrame(vif_data)\n",
        "df_vif.to_csv(f\"{WEEK4_DIR}/model3_vif.csv\", index=False)\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 7: PREDICTIONS & CORRECTIONS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"STEP 7: APPLYING STATISTICAL CORRECTIONS\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Predictions from OLS\n",
        "df_reg['predicted_bias_ols'] = ols_model.predict(X_simple)\n",
        "df_reg['era5_corrected_ols'] = df_reg['mean_era5_temp'] - df_reg['predicted_bias_ols']\n",
        "df_reg['error_corrected_ols'] = df_reg['era5_corrected_ols'] - df_reg['mean_station_temp']\n",
        "\n",
        "# Original performance\n",
        "original_rmse = df['rmse'].mean()\n",
        "ols_rmse = np.sqrt((df_reg['error_corrected_ols']**2).mean())\n",
        "ols_improvement = ((original_rmse - ols_rmse) / original_rmse) * 100\n",
        "\n",
        "print(f\"\\nüìä OLS Correction Performance:\")\n",
        "print(f\"  Original ERA5 RMSE: {original_rmse:.3f}¬∞C\")\n",
        "print(f\"  OLS-Corrected RMSE: {ols_rmse:.3f}¬∞C\")\n",
        "print(f\"  Improvement: {ols_improvement:+.1f}%\")\n",
        "\n",
        "# Mixed model predictions (if available)\n",
        "if mixed_available:\n",
        "    df_reg['predicted_bias_mixed'] = mixed_model.fittedvalues\n",
        "    df_reg['era5_corrected_mixed'] = df_reg['mean_era5_temp'] - df_reg['predicted_bias_mixed']\n",
        "    df_reg['error_corrected_mixed'] = df_reg['era5_corrected_mixed'] - df_reg['mean_station_temp']\n",
        "\n",
        "    mixed_rmse = np.sqrt((df_reg['error_corrected_mixed']**2).mean())\n",
        "    mixed_improvement = ((original_rmse - mixed_rmse) / original_rmse) * 100\n",
        "\n",
        "    print(f\"\\nüìä Mixed Model Correction Performance:\")\n",
        "    print(f\"  Mixed-Corrected RMSE: {mixed_rmse:.3f}¬∞C\")\n",
        "    print(f\"  Improvement: {mixed_improvement:+.1f}%\")\n",
        "\n",
        "# Save corrected dataset\n",
        "df_reg.to_csv(f\"{WEEK4_DIR}/model3_corrected_dataset.csv\", index=False)\n",
        "print(f\"\\n‚úì Saved corrected dataset: model3_corrected_dataset.csv\")\n",
        "\n",
        "# ============================================================================\n",
        "# VISUALIZATIONS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"STEP 8: CREATING VISUALIZATIONS\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Viz 1: Coefficient Plot with Confidence Intervals\n",
        "print(\"\\nCreating Viz 1: Coefficient Plot...\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 10))\n",
        "\n",
        "# Get OLS coefficients (exclude intercept and season dummies for clarity)\n",
        "coef_plot_vars = ['elevation_std', 'mean_station_temp_std', 'lat_std', 'lon_std',\n",
        "                  'distance_to_city_km_std', 'distance_to_coast_km_std', 'ndvi_mean_std']\n",
        "\n",
        "plot_data = []\n",
        "for var in coef_plot_vars:\n",
        "    if var in ols_model.params.index:\n",
        "        plot_data.append({\n",
        "            'Variable': var.replace('_std', ''),\n",
        "            'Coefficient': ols_model.params[var],\n",
        "            'CI_Lower': ols_model.conf_int().loc[var, 0],\n",
        "            'CI_Upper': ols_model.conf_int().loc[var, 1],\n",
        "            'P_Value': ols_model.pvalues[var]\n",
        "        })\n",
        "\n",
        "df_plot = pd.DataFrame(plot_data).sort_values('Coefficient')\n",
        "\n",
        "y_pos = np.arange(len(df_plot))\n",
        "colors = ['#e74c3c' if p < 0.05 else '#95a5a6' for p in df_plot['P_Value']]\n",
        "\n",
        "ax.barh(y_pos, df_plot['Coefficient'], xerr=[\n",
        "    df_plot['Coefficient'] - df_plot['CI_Lower'],\n",
        "    df_plot['CI_Upper'] - df_plot['Coefficient']\n",
        "], color=colors, alpha=0.7, edgecolor='black', linewidth=1.5,\n",
        "capsize=5, error_kw={'linewidth': 2})\n",
        "\n",
        "ax.axvline(0, color='black', linestyle='--', linewidth=2, alpha=0.7)\n",
        "ax.set_yticks(y_pos)\n",
        "ax.set_yticklabels(df_plot['Variable'])\n",
        "ax.set_xlabel('Standardized Coefficient (¬∞C per SD)', fontsize=13)\n",
        "ax.set_title('Model 3: OLS Regression Coefficients with 95% CI\\n(Red = Significant at p<0.05)',\n",
        "             fontsize=14, fontweight='bold')\n",
        "ax.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# Add significance markers\n",
        "for i, (coef, pval) in enumerate(zip(df_plot['Coefficient'], df_plot['P_Value'])):\n",
        "    if pval < 0.001:\n",
        "        sig = '***'\n",
        "    elif pval < 0.01:\n",
        "        sig = '**'\n",
        "    elif pval < 0.05:\n",
        "        sig = '*'\n",
        "    else:\n",
        "        sig = ''\n",
        "\n",
        "    if sig:\n",
        "        ax.text(coef + 0.02, i, sig, va='center', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{WEEK4_DIR}/viz1_model3_coefficients.png\", dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"  ‚úì Saved: viz1_model3_coefficients.png\")\n",
        "\n",
        "# Viz 2: Diagnostic Plots (4-panel)\n",
        "print(\"\\nCreating Viz 2: Diagnostic Plots...\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
        "\n",
        "# Panel A: Residuals vs Fitted\n",
        "ax = axes[0, 0]\n",
        "ax.scatter(fitted, residuals, alpha=0.5, s=30, c='steelblue', edgecolors='black', linewidth=0.5)\n",
        "ax.axhline(0, color='red', linestyle='--', linewidth=2)\n",
        "ax.set_xlabel('Fitted Values (¬∞C)', fontsize=11)\n",
        "ax.set_ylabel('Residuals (¬∞C)', fontsize=11)\n",
        "ax.set_title('A) Residuals vs Fitted\\n(Check for heteroscedasticity)', fontsize=12, fontweight='bold')\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Panel B: Q-Q Plot\n",
        "ax = axes[0, 1]\n",
        "stats.probplot(residuals, dist=\"norm\", plot=ax)\n",
        "ax.set_title('B) Normal Q-Q Plot\\n(Check normality assumption)', fontsize=12, fontweight='bold')\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Panel C: Scale-Location\n",
        "ax = axes[1, 0]\n",
        "standardized_resid = residuals / np.std(residuals)\n",
        "ax.scatter(fitted, np.sqrt(np.abs(standardized_resid)), alpha=0.5, s=30,\n",
        "          c='coral', edgecolors='black', linewidth=0.5)\n",
        "ax.set_xlabel('Fitted Values (¬∞C)', fontsize=11)\n",
        "ax.set_ylabel('‚àö|Standardized Residuals|', fontsize=11)\n",
        "ax.set_title('C) Scale-Location Plot\\n(Check homoscedasticity)', fontsize=12, fontweight='bold')\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Panel D: Residuals Histogram\n",
        "ax = axes[1, 1]\n",
        "ax.hist(residuals, bins=50, alpha=0.7, color='steelblue', edgecolor='black', density=True)\n",
        "\n",
        "# Overlay normal distribution\n",
        "mu, sigma = residuals.mean(), residuals.std()\n",
        "x = np.linspace(residuals.min(), residuals.max(), 100)\n",
        "ax.plot(x, stats.norm.pdf(x, mu, sigma), 'r-', linewidth=2, label='Normal Distribution')\n",
        "\n",
        "ax.axvline(0, color='black', linestyle='--', linewidth=2)\n",
        "ax.set_xlabel('Residuals (¬∞C)', fontsize=11)\n",
        "ax.set_ylabel('Density', fontsize=11)\n",
        "ax.set_title(f'D) Residual Distribution\\nMean: {mu:+.4f}¬∞C, Std: {sigma:.4f}¬∞C',\n",
        "             fontsize=12, fontweight='bold')\n",
        "ax.legend(fontsize=10)\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "fig.suptitle('Model 3: OLS Regression Diagnostics', fontsize=15, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{WEEK4_DIR}/viz2_model3_diagnostics.png\", dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"  ‚úì Saved: viz2_model3_diagnostics.png\")\n",
        "\n",
        "# Viz 3: Before vs After Correction\n",
        "print(\"\\nCreating Viz 3: Before/After Statistical Correction...\")\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
        "\n",
        "# Original\n",
        "ax = axes[0]\n",
        "scatter = ax.scatter(df_reg['mean_station_temp'], df_reg['mean_era5_temp'],\n",
        "                    c=df_reg['elevation'], cmap='terrain', s=50, alpha=0.6,\n",
        "                    edgecolors='black', linewidth=0.5)\n",
        "\n",
        "min_val = min(df_reg['mean_station_temp'].min(), df_reg['mean_era5_temp'].min())\n",
        "max_val = max(df_reg['mean_station_temp'].max(), df_reg['mean_era5_temp'].max())\n",
        "ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2,\n",
        "        label='Perfect Agreement', alpha=0.7)\n",
        "\n",
        "ax.set_xlabel('Observed Temperature (¬∞C)', fontsize=12)\n",
        "ax.set_ylabel('ERA5 Temperature (¬∞C)', fontsize=12)\n",
        "ax.set_title(f'Original ERA5\\nRMSE: {original_rmse:.3f}¬∞C',\n",
        "             fontsize=13, fontweight='bold')\n",
        "ax.legend(fontsize=10)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# OLS Corrected\n",
        "ax = axes[1]\n",
        "scatter = ax.scatter(df_reg['mean_station_temp'], df_reg['era5_corrected_ols'],\n",
        "                    c=df_reg['elevation'], cmap='terrain', s=50, alpha=0.6,\n",
        "                    edgecolors='black', linewidth=0.5)\n",
        "\n",
        "min_val = min(df_reg['mean_station_temp'].min(), df_reg['era5_corrected_ols'].min())\n",
        "max_val = max(df_reg['mean_station_temp'].max(), df_reg['era5_corrected_ols'].max())\n",
        "ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2,\n",
        "        label='Perfect Agreement', alpha=0.7)\n",
        "\n",
        "ax.set_xlabel('Observed Temperature (¬∞C)', fontsize=12)\n",
        "ax.set_ylabel('Corrected ERA5 Temperature (¬∞C)', fontsize=12)\n",
        "ax.set_title(f'OLS-Corrected ERA5\\nRMSE: {ols_rmse:.3f}¬∞C ({ols_improvement:+.1f}%)',\n",
        "             fontsize=13, fontweight='bold')\n",
        "ax.legend(fontsize=10)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "fig.colorbar(scatter, ax=axes, label='Elevation (m)', orientation='horizontal', pad=0.1)\n",
        "\n",
        "fig.suptitle('Model 3: Impact of Statistical Regression Correction',\n",
        "             fontsize=15, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{WEEK4_DIR}/viz3_model3_correction_impact.png\", dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"  ‚úì Saved: viz3_model3_correction_impact.png\")\n",
        "\n",
        "# Viz 4: Model Comparison (if multiple models available)\n",
        "if mixed_available:\n",
        "    print(\"\\nCreating Viz 4: Model Comparison...\")\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 7))\n",
        "\n",
        "    models_comparison = [\n",
        "        {'Model': 'OLS', 'RMSE': ols_rmse, 'R2': ols_results['R2']},\n",
        "    ]\n",
        "\n",
        "    if mixed_available:\n",
        "        models_comparison.append({\n",
        "            'Model': 'Mixed Effects',\n",
        "            'RMSE': mixed_rmse,\n",
        "            'R2': np.nan  # Mixed models don't have standard R¬≤\n",
        "        })\n",
        "\n",
        "    df_comparison = pd.DataFrame(models_comparison)\n",
        "\n",
        "    x_pos = np.arange(len(df_comparison))\n",
        "    colors = ['#3498db', '#e74c3c'][:len(df_comparison)]\n",
        "\n",
        "    bars = ax.bar(x_pos, df_comparison['RMSE'], color=colors, alpha=0.7,\n",
        "                  edgecolor='black', linewidth=1.5)\n",
        "\n",
        "    # Add original RMSE line\n",
        "    ax.axhline(original_rmse, color='red', linestyle='--', linewidth=2,\n",
        "              label=f'Original: {original_rmse:.3f}¬∞C', alpha=0.7)\n",
        "\n",
        "    # Add values\n",
        "    for bar, rmse in zip(bars, df_comparison['RMSE']):\n",
        "        height = bar.get_height()\n",
        "        improvement = ((original_rmse - rmse) / original_rmse) * 100\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.05,\n",
        "                f'{rmse:.3f}¬∞C\\n({improvement:+.1f}%)',\n",
        "                ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
        "\n",
        "    ax.set_xticks(x_pos)\n",
        "    ax.set_xticklabels(df_comparison['Model'])\n",
        "    ax.set_ylabel('RMSE (¬∞C)', fontsize=13)\n",
        "    ax.set_title('Model 3: Statistical Model Comparison', fontsize=14, fontweight='bold')\n",
        "    ax.legend(fontsize=11)\n",
        "    ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{WEEK4_DIR}/viz4_model3_comparison.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(\"  ‚úì Saved: viz4_model3_comparison.png\")\n",
        "\n",
        "# ============================================================================\n",
        "# FINAL SUMMARY\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODEL 3: STATISTICAL REGRESSION MODEL - COMPLETE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nüìä MODEL 1: ORDINARY LEAST SQUARES\")\n",
        "print(f\"  R¬≤: {ols_results['R2']:.4f}\")\n",
        "print(f\"  Adjusted R¬≤: {ols_results['Adj_R2']:.4f}\")\n",
        "print(f\"  RMSE: {ols_results['RMSE']:.3f}¬∞C\")\n",
        "print(f\"  AIC: {ols_results['AIC']:.1f}\")\n",
        "print(f\"  Observations: {ols_results['N_obs']}\")\n",
        "\n",
        "if mixed_available:\n",
        "    print(\"\\nüìä MODEL 2: LINEAR MIXED EFFECTS\")\n",
        "    print(f\"  AIC: {mixed_results['AIC']:.1f}\")\n",
        "    print(f\"  BIC: {mixed_results['BIC']:.1f}\")\n",
        "    print(f\"  Log-Likelihood: {mixed_results['Log_Likelihood']:.2f}\")\n",
        "\n",
        "print(\"\\nüéØ CORRECTION PERFORMANCE:\")\n",
        "print(f\"  Original ERA5 RMSE: {original_rmse:.3f}¬∞C\")\n",
        "print(f\"  OLS-Corrected RMSE: {ols_rmse:.3f}¬∞C\")\n",
        "print(f\"  Improvement: {ols_improvement:+.1f}%\")\n",
        "\n",
        "if mixed_available:\n",
        "    print(f\"  Mixed-Corrected RMSE: {mixed_rmse:.3f}¬∞C\")\n",
        "    print(f\"  Improvement: {mixed_improvement:+.1f}%\")\n",
        "\n",
        "print(\"\\nüî¨ MODEL DIAGNOSTICS:\")\n",
        "print(f\"  Normality (Jarque-Bera): {'‚úì Pass' if jb_pval > 0.05 else '‚ö†Ô∏è  Borderline'}\")\n",
        "print(f\"  Homoscedasticity (BP): {'‚úì Pass' if bp_pval > 0.05 else '‚ö†Ô∏è  Some heteroscedasticity'}\")\n",
        "print(f\"  Multicollinearity: {'‚úì Low VIF' if all(df_vif['VIF'] < 10) else '‚ö†Ô∏è  Some collinearity'}\")\n",
        "\n",
        "print(\"\\nüèÜ MOST SIGNIFICANT PREDICTORS:\")\n",
        "significant_vars = df_coefs[(df_coefs['Model'] == 'OLS') & (df_coefs['P_Value'] < 0.05)].sort_values('P_Value')\n",
        "for i, row in significant_vars.head(5).iterrows():\n",
        "    print(f\"  {i+1}. {row['Variable']:<30} (coef={row['Coefficient']:+.4f}, p={row['P_Value']:.6f})\")\n",
        "\n",
        "print(\"\\nüìÅ OUTPUTS SAVED:\")\n",
        "print(\"  1. model3_ols_summary.txt - Full OLS regression output\")\n",
        "if mixed_available:\n",
        "    print(\"  2. model3_mixed_summary.txt - Mixed effects model output\")\n",
        "if quantreg_available:\n",
        "    print(\"  3. model3_quantreg_summary.txt - Quantile regression output\")\n",
        "print(\"  4. model3_coefficients_comparison.csv - All model coefficients\")\n",
        "print(\"  5. model3_vif.csv - Multicollinearity diagnostics\")\n",
        "print(\"  6. model3_corrected_dataset.csv - Corrected temperatures\")\n",
        "print(\"  7. viz1_model3_coefficients.png - Coefficient plot with CI\")\n",
        "print(\"  8. viz2_model3_diagnostics.png - 4-panel diagnostics\")\n",
        "print(\"  9. viz3_model3_correction_impact.png - Before/after correction\")\n",
        "if mixed_available:\n",
        "    print(\" 10. viz4_model3_comparison.png - Model comparison\")\n",
        "\n",
        "print(\"\\nüí° INTERPRETATION GUIDE:\")\n",
        "print(\"  ‚Ä¢ Standardized coefficients show effect per 1 SD change in predictor\")\n",
        "print(\"  ‚Ä¢ Positive coefficient = increases bias (ERA5 becomes warmer)\")\n",
        "print(\"  ‚Ä¢ Negative coefficient = decreases bias (ERA5 becomes colder)\")\n",
        "print(\"  ‚Ä¢ P-values test if effect is statistically different from zero\")\n",
        "print(\"  ‚Ä¢ R¬≤ shows proportion of variance explained by the model\")\n",
        "print(\"  ‚Ä¢ Mixed effects account for city-level clustering\")\n",
        "\n",
        "print(\"\\n‚úì Model 3 analysis complete!\")\n",
        "print(\"\\n\" + \"=\"*80)\n"
      ],
      "metadata": {
        "id": "_yG1girW_i2O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a104f8e-8724-4d10-8024-290583bb2122"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "WEEK 4 - MODEL 3: STATISTICAL REGRESSION MODEL\n",
            "================================================================================\n",
            "\n",
            "Loaded 286 stations from Week 3\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "STEP 1: DATA PREPARATION FOR REGRESSION\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "‚úì Clean regression dataset: 286 observations\n",
            "\n",
            "Standardizing continuous variables for interpretation...\n",
            "  elevation                : mean=457.71, std=450.81\n",
            "  lat                      : mean=46.52, std=4.87\n",
            "  lon                      : mean=7.37, std=8.09\n",
            "  distance_to_city_km      : mean=92.01, std=42.72\n",
            "  distance_to_coast_km     : mean=556.93, std=193.03\n",
            "  mean_station_temp        : mean=17.67, std=2.99\n",
            "  ndvi_mean                : mean=0.44, std=0.15\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "STEP 2: SIMPLE LINEAR REGRESSION (BASELINE)\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Fitting Ordinary Least Squares model...\n",
            "\n",
            "============================================================\n",
            "MODEL 1: SIMPLE LINEAR REGRESSION RESULTS\n",
            "============================================================\n",
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                   bias   R-squared:                       0.267\n",
            "Model:                            OLS   Adj. R-squared:                  0.240\n",
            "Method:                 Least Squares   F-statistic:                     10.01\n",
            "Date:                Thu, 04 Dec 2025   Prob (F-statistic):           2.52e-14\n",
            "Time:                        04:05:01   Log-Likelihood:                -403.83\n",
            "No. Observations:                 286   AIC:                             829.7\n",
            "Df Residuals:                     275   BIC:                             869.9\n",
            "Df Model:                          10                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "============================================================================================\n",
            "                               coef    std err          t      P>|t|      [0.025      0.975]\n",
            "--------------------------------------------------------------------------------------------\n",
            "const                       -1.5063      0.349     -4.313      0.000      -2.194      -0.819\n",
            "elevation_std               -1.7175      0.302     -5.686      0.000      -2.312      -1.123\n",
            "mean_station_temp_std       -2.1862      0.294     -7.430      0.000      -2.765      -1.607\n",
            "lat_std                     -2.3472      0.467     -5.026      0.000      -3.267      -1.428\n",
            "lon_std                     -0.3573      0.207     -1.729      0.085      -0.764       0.049\n",
            "distance_to_city_km_std     -0.0812      0.065     -1.251      0.212      -0.209       0.047\n",
            "distance_to_coast_km_std    -0.1753      0.105     -1.673      0.096      -0.382       0.031\n",
            "ndvi_mean_std               -0.0381      0.065     -0.588      0.557      -0.166       0.089\n",
            "season_Spring                0.1396      0.364      0.384      0.702      -0.577       0.856\n",
            "season_Summer                0.5379      0.371      1.451      0.148      -0.192       1.268\n",
            "season_Winter               -0.3708      0.472     -0.785      0.433      -1.300       0.559\n",
            "==============================================================================\n",
            "Omnibus:                       48.958   Durbin-Watson:                   1.842\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              169.201\n",
            "Skew:                          -0.687   Prob(JB):                     1.81e-37\n",
            "Kurtosis:                       6.509   Cond. No.                         21.4\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
            "\n",
            "‚úì Saved OLS summary: model3_ols_summary.txt\n",
            "\n",
            "üìä Model Statistics:\n",
            "  R¬≤: 0.2669\n",
            "  Adjusted R¬≤: 0.2403\n",
            "  RMSE: 1.013¬∞C\n",
            "  AIC: 829.66\n",
            "  BIC: 869.87\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "STEP 3: LINEAR MIXED EFFECTS MODEL (HIERARCHICAL)\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Fitting Mixed Effects Model with city as random effect...\n",
            "  Fixed effects: elevation, temperature, lat, lon, distances, NDVI, season\n",
            "  Random effect: city (accounts for city-to-city variation)\n",
            "\n",
            "============================================================\n",
            "MODEL 2: LINEAR MIXED EFFECTS MODEL RESULTS\n",
            "============================================================\n",
            "               Mixed Linear Model Regression Results\n",
            "===================================================================\n",
            "Model:                MixedLM     Dependent Variable:     bias     \n",
            "No. Observations:     286         Method:                 REML     \n",
            "No. Groups:           5           Scale:                  0.9830   \n",
            "Min. group size:      5           Log-Likelihood:         -413.7650\n",
            "Max. group size:      88          Converged:              Yes      \n",
            "Mean group size:      57.2                                         \n",
            "-------------------------------------------------------------------\n",
            "                         Coef.  Std.Err.   z    P>|z| [0.025 0.975]\n",
            "-------------------------------------------------------------------\n",
            "Intercept                -1.694    0.527 -3.217 0.001 -2.727 -0.662\n",
            "C(season)[T.Spring]       0.211    0.358  0.591 0.555 -0.490  0.912\n",
            "C(season)[T.Summer]       0.952    0.391  2.433 0.015  0.185  1.719\n",
            "C(season)[T.Winter]      -0.211    0.468 -0.451 0.652 -1.128  0.706\n",
            "elevation_std            -1.789    0.299 -5.973 0.000 -2.376 -1.202\n",
            "mean_station_temp_std    -2.292    0.297 -7.721 0.000 -2.874 -1.710\n",
            "lat_std                  -3.092    0.647 -4.776 0.000 -4.361 -1.823\n",
            "lon_std                   0.212    0.437  0.484 0.628 -0.646  1.069\n",
            "distance_to_city_km_std  -0.063    0.065 -0.975 0.329 -0.191  0.064\n",
            "distance_to_coast_km_std  0.064    0.178  0.358 0.721 -0.285  0.412\n",
            "ndvi_mean_std            -0.016    0.065 -0.249 0.803 -0.143  0.111\n",
            "Group Var                 0.658    0.707                           \n",
            "===================================================================\n",
            "\n",
            "\n",
            "‚úì Saved Mixed Model summary: model3_mixed_summary.txt\n",
            "\n",
            "üìä Model Statistics:\n",
            "  AIC: nan\n",
            "  BIC: nan\n",
            "  Log-Likelihood: -413.77\n",
            "\n",
            "üé≤ Random Effects:\n",
            "  City variance: 0.6584\n",
            "  Residual variance: 0.9830\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "STEP 4: QUANTILE REGRESSION (ROBUST TO OUTLIERS)\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Fitting Quantile Regression at median (50th percentile)...\n",
            "  (Robust alternative to OLS, less sensitive to outliers)\n",
            "\n",
            "============================================================\n",
            "MODEL 3: QUANTILE REGRESSION RESULTS (Median)\n",
            "============================================================\n",
            "                         QuantReg Regression Results                          \n",
            "==============================================================================\n",
            "Dep. Variable:                   bias   Pseudo R-squared:               0.1499\n",
            "Model:                       QuantReg   Bandwidth:                      0.2967\n",
            "Method:                 Least Squares   Sparsity:                       0.9648\n",
            "Date:                Thu, 04 Dec 2025   No. Observations:                  286\n",
            "Time:                        04:05:01   Df Residuals:                      275\n",
            "                                        Df Model:                           10\n",
            "============================================================================================\n",
            "                               coef    std err          t      P>|t|      [0.025      0.975]\n",
            "--------------------------------------------------------------------------------------------\n",
            "const                       -1.6318      0.166     -9.809      0.000      -1.959      -1.304\n",
            "elevation_std               -1.5884      0.144    -11.039      0.000      -1.872      -1.305\n",
            "mean_station_temp_std       -1.8434      0.140    -13.152      0.000      -2.119      -1.567\n",
            "lat_std                     -2.2674      0.222    -10.192      0.000      -2.705      -1.829\n",
            "lon_std                     -0.1383      0.098     -1.405      0.161      -0.332       0.055\n",
            "distance_to_city_km_std     -0.0196      0.031     -0.632      0.528      -0.080       0.041\n",
            "distance_to_coast_km_std    -0.1413      0.050     -2.831      0.005      -0.240      -0.043\n",
            "ndvi_mean_std               -0.0146      0.031     -0.474      0.636      -0.075       0.046\n",
            "season_Spring                0.4927      0.173      2.842      0.005       0.151       0.834\n",
            "season_Summer                0.5761      0.177      3.262      0.001       0.228       0.924\n",
            "season_Winter                0.3223      0.225      1.433      0.153      -0.120       0.765\n",
            "============================================================================================\n",
            "\n",
            "‚úì Saved Quantile Regression summary: model3_quantreg_summary.txt\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "STEP 5: COEFFICIENT COMPARISON & INTERPRETATION\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "‚úì Saved coefficient comparison: model3_coefficients_comparison.csv\n",
            "\n",
            "üìä KEY COEFFICIENTS (OLS Model):\n",
            "Variable                       Coef       Std Err    P-value    Sig  \n",
            "----------------------------------------------------------------------\n",
            "   elevation_std                   -1.7175    0.3021  0.000000  ***  \n",
            "   mean_station_temp_std           -2.1862    0.2942  0.000000  ***  \n",
            "   ndvi_mean_std                   -0.0381    0.0648  0.556933       \n",
            "   distance_to_city_km_std         -0.0812    0.0649  0.212007       \n",
            "   lat_std                         -2.3472    0.4670  0.000001  ***  \n",
            "\n",
            "Significance codes: *** p<0.001, ** p<0.01, * p<0.05\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "STEP 6: MODEL DIAGNOSTICS\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "1. Normality Test (Jarque-Bera):\n",
            "   Test statistic: 169.2012\n",
            "   P-value: 0.000000\n",
            "   ‚ö†Ô∏è  Residuals may not be perfectly normal (but OLS is robust)\n",
            "\n",
            "2. Homoscedasticity Test (Breusch-Pagan):\n",
            "   Test statistic: 65.9147\n",
            "   P-value: 0.000000\n",
            "   ‚ö†Ô∏è  Some heteroscedasticity detected (common in real data)\n",
            "\n",
            "3. Multicollinearity Check (VIF):\n",
            "   Variable                       VIF        Status              \n",
            "   ------------------------------------------------------------\n",
            "   elevation_std                     25.35  ‚ùå High              \n",
            "   mean_station_temp_std             24.06  ‚ùå High              \n",
            "   lat_std                           60.61  ‚ùå High              \n",
            "   lon_std                           11.86  ‚ùå High              \n",
            "   distance_to_city_km_std            1.17  ‚úì Low               \n",
            "   distance_to_coast_km_std           3.05  ‚úì Low               \n",
            "   ndvi_mean_std                      1.17  ‚úì Low               \n",
            "   season_Spring                      8.95  ‚ö†Ô∏è  Moderate        \n",
            "   season_Summer                      8.63  ‚ö†Ô∏è  Moderate        \n",
            "   season_Winter                      2.30  ‚úì Low               \n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "STEP 7: APPLYING STATISTICAL CORRECTIONS\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "üìä OLS Correction Performance:\n",
            "  Original ERA5 RMSE: 2.048¬∞C\n",
            "  OLS-Corrected RMSE: 0.993¬∞C\n",
            "  Improvement: +51.5%\n",
            "\n",
            "üìä Mixed Model Correction Performance:\n",
            "  Mixed-Corrected RMSE: 0.968¬∞C\n",
            "  Improvement: +52.7%\n",
            "\n",
            "‚úì Saved corrected dataset: model3_corrected_dataset.csv\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "STEP 8: CREATING VISUALIZATIONS\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Creating Viz 1: Coefficient Plot...\n",
            "  ‚úì Saved: viz1_model3_coefficients.png\n",
            "\n",
            "Creating Viz 2: Diagnostic Plots...\n",
            "  ‚úì Saved: viz2_model3_diagnostics.png\n",
            "\n",
            "Creating Viz 3: Before/After Statistical Correction...\n",
            "  ‚úì Saved: viz3_model3_correction_impact.png\n",
            "\n",
            "Creating Viz 4: Model Comparison...\n",
            "  ‚úì Saved: viz4_model3_comparison.png\n",
            "\n",
            "================================================================================\n",
            "MODEL 3: STATISTICAL REGRESSION MODEL - COMPLETE\n",
            "================================================================================\n",
            "\n",
            "üìä MODEL 1: ORDINARY LEAST SQUARES\n",
            "  R¬≤: 0.2669\n",
            "  Adjusted R¬≤: 0.2403\n",
            "  RMSE: 1.013¬∞C\n",
            "  AIC: 829.7\n",
            "  Observations: 286\n",
            "\n",
            "üìä MODEL 2: LINEAR MIXED EFFECTS\n",
            "  AIC: nan\n",
            "  BIC: nan\n",
            "  Log-Likelihood: -413.77\n",
            "\n",
            "üéØ CORRECTION PERFORMANCE:\n",
            "  Original ERA5 RMSE: 2.048¬∞C\n",
            "  OLS-Corrected RMSE: 0.993¬∞C\n",
            "  Improvement: +51.5%\n",
            "  Mixed-Corrected RMSE: 0.968¬∞C\n",
            "  Improvement: +52.7%\n",
            "\n",
            "üî¨ MODEL DIAGNOSTICS:\n",
            "  Normality (Jarque-Bera): ‚ö†Ô∏è  Borderline\n",
            "  Homoscedasticity (BP): ‚ö†Ô∏è  Some heteroscedasticity\n",
            "  Multicollinearity: ‚ö†Ô∏è  Some collinearity\n",
            "\n",
            "üèÜ MOST SIGNIFICANT PREDICTORS:\n",
            "  2. mean_station_temp_std          (coef=-2.1862, p=0.000000)\n",
            "  1. elevation_std                  (coef=-1.7175, p=0.000000)\n",
            "  3. lat_std                        (coef=-2.3472, p=0.000001)\n",
            "\n",
            "üìÅ OUTPUTS SAVED:\n",
            "  1. model3_ols_summary.txt - Full OLS regression output\n",
            "  2. model3_mixed_summary.txt - Mixed effects model output\n",
            "  3. model3_quantreg_summary.txt - Quantile regression output\n",
            "  4. model3_coefficients_comparison.csv - All model coefficients\n",
            "  5. model3_vif.csv - Multicollinearity diagnostics\n",
            "  6. model3_corrected_dataset.csv - Corrected temperatures\n",
            "  7. viz1_model3_coefficients.png - Coefficient plot with CI\n",
            "  8. viz2_model3_diagnostics.png - 4-panel diagnostics\n",
            "  9. viz3_model3_correction_impact.png - Before/after correction\n",
            " 10. viz4_model3_comparison.png - Model comparison\n",
            "\n",
            "üí° INTERPRETATION GUIDE:\n",
            "  ‚Ä¢ Standardized coefficients show effect per 1 SD change in predictor\n",
            "  ‚Ä¢ Positive coefficient = increases bias (ERA5 becomes warmer)\n",
            "  ‚Ä¢ Negative coefficient = decreases bias (ERA5 becomes colder)\n",
            "  ‚Ä¢ P-values test if effect is statistically different from zero\n",
            "  ‚Ä¢ R¬≤ shows proportion of variance explained by the model\n",
            "  ‚Ä¢ Mixed effects account for city-level clustering\n",
            "\n",
            "‚úì Model 3 analysis complete!\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RrP0qyCihb-L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}